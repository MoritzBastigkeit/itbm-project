% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{humannat}
\usepackage{titling}
\pretitle{\begin{center}\Large}
\posttitle{\par\end{center}}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Predictive Analytics: Application in the Credit Risk Domain: Case Study Teaching (CST)-Vignette in Cheat Sheet Style (Group Project Cover Sheet)},
  pdfauthor={Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Predictive Analytics: Application in the Credit Risk Domain: Case Study Teaching (CST)-Vignette in Cheat Sheet Style (Group Project Cover Sheet)}
\author{Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha}
\date{Nov.~27, 2025 (Scorecard\_Intro\_2511.Rmd)}

\begin{document}
\maketitle

\pagebreak

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\section{Introduction}\label{introduction}

A central function of banks is to provide loans to individuals and companies. Credit scoring models are an essential tool for banks to assess the creditworthiness of lenders and predict how likely they are to meet their financial obligations. Popular models are based on the 3C's, 4C's, or 5C's, which stand for character, capital, collateral, capacity, and condition. However, with advancing technology, more novel approaches have emerged. In the subsequent paper, a credit scoring model is built and evaluated based on German data. The objective of the paper is to investigate the predictive value of demographic attributes. The objective of this study is not to enhance accuracy, but rather to offer insight into the structure of creditworthiness within the German context.

\section{Methodology}\label{methodology}

In order to build and assess these models, the Weight-of-Evidence (WoE) approach was taken. In it, the raw features of individuals are transformed in that they are sorted in bins based on their predictive strength to distinguish between defaulters and non-defaulters. To calculate it, the logarithm of the ratio of non-defaulters to defaulters within a group is taken, resulting in a score that represents credit risk. In doing so, the WoE approach allows to convert categorical or numerical values into values that are then processed in a logistic regression or scorecard model. In the underlying general formula, the event is defined in the underlying case as being a \textit{good customer}, meaning that this individual repays their debt towards the bank, respectively in each group \(i\).
\begin{equation}
\label{woe_eq}
WoE_i = \ln\left(\frac{Event_i\%}{NonEvent_i\%}\right)
\end{equation}
The resulting predictor variables were then used in two models, first a logistic regression and secondly a scorecard model, to compare the accuracy of each. Both models aim to predict the probability of default \(P(Y=1)\) for the given set of predictors with the logistic approach using a sigmoid function. Sigmoid functions are mathematical functions, usually shaped like an S, used for classification problems by taking real numbers as an input and transforming them to values between 0 and 1 (probabilities). The model takes the form\[P(default=1|X) = \frac{1}{1+e^{-(\beta_0+\beta_1X_1+...+\beta_kX_k)}}\] where the coefficients \(\beta\) represent the individual contribution of each variable to the likelihood of default.

On the other hand, a scorecard model was implemented to serve as a more transparent comparison to the logistic regression. After the variables are initially binned and converted into their respective WoE values, each bin of each variable was assigned a fixed number of score points that reflect its contribution to the credit risk. This was done by using the \(scorecard()\) function from the eponymous package in R. For any individual applicant, the total credit score was then calculated by summing up the score points across their variables with higher scores representing lower risk of defaulting.

To evaluate and the performance of these models, a selected number of accuracy metrics, namely Area Under the ROC Curve (AUC) and the Gini coefficient, was used. These metrics measure how well the model is able to differentiate between defaulters and non-defaulters. Furthermore, the Root Mean Squared Error (RSME) quantifies the models' accuracy in their predicted probabilities by quantifying the average deviation between predicted and actual outcomes (in the train set). Additionally, matrices summarizing the performance by reporting true positives, false positives, true negatives and false negatives were generated (confusion matrices). The stability or robustness of the models' scores and probability distributions were assessed with the Population Stability Index (PSI). This metric compares the distributions in the training set with those of the validation set to detect shifts in population characteristics across different samples. In this context, a low score is indicative of a stable model whereas high values suggest a potential drift or deterioration.

\section{Data Selection}\label{data-selection}

To train the models to accurately predict the creditworthiness, it is imperative to select predictor variables based on their relevance to the individuals' financial behaviour.

To get a better understanding of the data structure, Table 1 gives an insight into the German dataset.

\begin{table}[!h]
\centering
\caption{\label{tab:dataTable}Exemplary View of the Data Table}
\centering
\begin{tabular}[t]{lcc}
\toprule
creditability & status.of.existing.checking.account & duration.in.month\\
\midrule
good & ... < 0 DM & 6\\
bad & 0 <= ... < 200 DM & 48\\
good & no checking account & 12\\
good & ... < 0 DM & 42\\
bad & ... < 0 DM & 24\\
\addlinespace
good & no checking account & 36\\
\bottomrule
\end{tabular}
\end{table}

The selection process focused on variables that reflect past repayment behaviour, financial stability, and loan characteristics. Restricting the model to these input variables aims to produce transparent and robust results that reflect the real world risk modeling practices. To do so, each variable from the German dataset was evaluated on its Information Value (IV) to determine how well they are suited in discriminating between good and bad borrowers (data quality). Variables with high scores were retained for the subsequent modeling and further investigated in their underlying structuring. In practical terms, a higher IV relates to stronger predictive power of the underlying variable, with general thresholds of being good predictors of values larger than 0.3. Table 2 visualizes the calculated IV scores for the selected variables.

The five selected predictor variables are:

\begin{itemize}

\item \textbf{status.of.existing.checking.account} (categorical)
This is a categorical variable that describes the applicant's checking account condition using qualitative labels such as "no checking account", "<0 DM", etc. 

\item \textbf{duration.in.month} (numeric) \\
This is a numeric variable indicating the length of the loan contract in months, reflecting how long the applicant will take to repay off the credit.

\item \textbf{credit.history} (categorical) \\
This is another categorical variable that describes the applicant's past repayment behaviour, ranging from values of "no credits taken" and "all credits in this bank paid back duly" to "critical account".

\item \textbf{savings.account.and.bonds} (categorical) \\
This is another variable that categorises the applicant's savings level, both in their savings account and bonds. There are several categories such as "unknown/no savings account" to " < 100 DM" to separate those into groups.

\item \textbf{purpose} (categorical) \\
Purpose reflects a categorical variable that gives insight into what the applicant intends to do with the credit. Values range from "used car" and "new car" to "education", etc. 
\end{itemize}

\begin{table}[!h]
\centering
\caption{\label{tab:dataIVTable}Information Value of Variables}
\centering
\begin{tabular}[t]{lc}
\toprule
Variable & Information Value\\
\midrule
\cellcolor{gray!10}{status.of.existing.checking.account} & \cellcolor{gray!10}{0.67}\\
duration.in.month & 0.33\\
\cellcolor{gray!10}{credit.history} & \cellcolor{gray!10}{0.29}\\
savings.account.and.bonds & 0.20\\
\cellcolor{gray!10}{purpose} & \cellcolor{gray!10}{0.17}\\
\bottomrule
\end{tabular}
\end{table}

In the following steps, the data was filtered to exclude rows with missing values and split into train and validation set. The split chosen was at a ratio of .75 to .25. This is being done to have two independent samples for training and validating the model at a later point.

\subsection{Specifying dummy variable for credit defaults: default.list}\label{specifying-dummy-variable-for-credit-defaults-default.list}

For being able to statistically analyze and test the results from the
scorecard the default.list is established that contains the default
values of the response variable

\textbf{Hint}: The default.list is needed for calculating the population
stability indes (PSI) with the function perf\_psi() and the gains table
with the function gains\_table().

\section{Weight-Of-Evidence (WoE)-based transformation of predictor variables}\label{weight-of-evidence-woe-based-transformation-of-predictor-variables}

\subsection{WoE-based binning of train and validate samples: bins.list}\label{woe-based-binning-of-train-and-validate-samples-bins.list}

WoE-based classing, i.e.~binning and grouping of predictor variables

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## v Binning on 727 rows and 6 columns in 00:00:00
\end{verbatim}

\textbf{Hint}: The default binning method is method=``width''. Other methods are

\begin{itemize}
\item
  ``frequ'' that support numerical variables as well as
\item
  ``tree'' and ``chimerge'' supporting both, i.e.~numerical and
  categorical variables which are used in the optimal binning
  approach.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "status.of.existing.checking.account" "duration.in.month"                  
## [3] "credit.history"                      "savings.account.and.bonds"          
## [5] "purpose"
\end{verbatim}

Plotting the bins (including bin statistics)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{status.of.existing.checking.account }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $status.of.existing.checking.account
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-6-1} \end{center}

\textbf{Hint}: credit.amount does not have an acceptable structure of the default rates (positive probability) over the bins like e.g.~a linear or u-curve structure; hence it should not be included in the scorecard model!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{duration.in.month }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $duration.in.month
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{credit.history }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $credit.history
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{savings.account.and.bonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $savings.account.and.bonds
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{purpose }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $purpose
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-10-1} \end{center}

\textbf{Hint}: duration.in.month has lineare structure of the default rates; hence it should be included in the scorecard model!

\textbf{Excursion}: Manual bin-adjustments

Bins can be altered manually by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Saving the bin list generated in the woebin() function via e.g.
  save\_as=``breaks2410.list''
\item
  Loading the saved R-file ``breaks2410.list.R'', editing the breaks as
  needed and storing the file
\item
  Sourcing the edited and stored ``breaks2410.list.R'' file with the
  ``source(\ldots)\$value'' function
\item
  Binning the data again with the ``woebin()'' function with the
  additional argument ``break\_list''
\end{enumerate}

ad 1)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{,}
         \AttributeTok{save\_as =} \StringTok{"breaks2410.list"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

ad 3)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{breaksList }\OtherTok{\textless{}{-}} \FunctionTok{source}\NormalTok{(}\StringTok{"breaks2410.list.R"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{value}
\end{Highlighting}
\end{Shaded}

ad 4)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{,}
         \AttributeTok{breaks\_list =} \StringTok{"breaksList"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: The above code chunks are not yet evaluated, as they are
performed only when the original binning does not deliver beneficial
results.

\subsection{WoE-based transforming of predictor variables: data\_woe.list}\label{woe-based-transforming-of-predictor-variables-data_woe.list}

\subsubsection{WoE-based transforming of train and validate data: data\_woe.list}\label{woe-based-transforming-of-train-and-validate-data-data_woe.list}

Transforming splitted sample: Needed for train/validate analysis

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{woebin\_ply}\NormalTok{(x, bins.list))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## v Woe transformating on 727 rows and 5 columns in 00:00:00
\end{verbatim}

\begin{verbatim}
## v Woe transformating on 273 rows and 5 columns in 00:00:10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{lapply}\NormalTok{(class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
## [1] "data.table" "data.frame"
## 
## $validate
## [1] "data.table" "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{lapply}\NormalTok{(dim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
## [1] 727   6
## 
## $validate
## [1] 273   6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#data\_woe.list$train \%\textgreater{}\% }
\CommentTok{\#  select(creditability, credit.amount\_woe, duration.in.month\_woe) \%\textgreater{}\% }
\CommentTok{\#  head()}
\end{Highlighting}
\end{Shaded}

\newpage

\section{Generalized linear model (glm): Regressing predictors against responses}\label{generalized-linear-model-glm-regressing-predictors-against-responses}

\subsection{Logistic regression of WoE-transformed predictors: glm(.,data\_woe.list\$train)}\label{logistic-regression-of-woe-transformed-predictors-glm.data_woe.listtrain}

The WoE-based logistic regression is the preferred regression approach
as it delivers the most compact regression models.

\subsubsection{Constructing and calibrating the WoE-based logistic regression model}\label{constructing-and-calibrating-the-woe-based-logistic-regression-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(creditability }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(), }
                    \AttributeTok{data =}\NormalTok{ data\_woe.list}\SpecialCharTok{$}\NormalTok{train) }
\end{Highlighting}
\end{Shaded}

\subsubsection{Investigating the fitted regression model}\label{investigating-the-fitted-regression-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.glm}\SpecialCharTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 711.2915
\end{verbatim}

Summary of regression: summary()

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.glm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = creditability ~ ., family = binomial(), data = data_woe.list$train)
## 
## Coefficients:
##                                         Estimate Std. Error z value Pr(>|z|)
## (Intercept)                             -0.84422    0.09532  -8.857  < 2e-16
## status.of.existing.checking.account_woe  0.83286    0.12266   6.790 1.12e-11
## duration.in.month_woe                    0.96608    0.17657   5.471 4.46e-08
## credit.history_woe                       0.78853    0.16945   4.653 3.26e-06
## savings.account.and.bonds_woe            0.86020    0.24723   3.479 0.000503
## purpose_woe                              0.94490    0.22095   4.277 1.90e-05
##                                            
## (Intercept)                             ***
## status.of.existing.checking.account_woe ***
## duration.in.month_woe                   ***
## credit.history_woe                      ***
## savings.account.and.bonds_woe           ***
## purpose_woe                             ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 886.32  on 726  degrees of freedom
## Residual deviance: 699.29  on 721  degrees of freedom
## AIC: 711.29
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

\subsection{Logistic regression of original predictors: glm(.,data\_f.list\$train)}\label{logistic-regression-of-original-predictors-glm.data_f.listtrain}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#data\_f.glm \textless{}{-} glm(creditability \textasciitilde{} ., }
 \CommentTok{\#                 family = binomial(), }
  \CommentTok{\#                data = data\_f.list$train \%\textgreater{}\% }
   \CommentTok{\#                 select(creditability,}
     \CommentTok{\#                      credit.amount,}
    \CommentTok{\#                       duration.in.month,}
   \CommentTok{\#                        credit.history)) }
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: For simplicity only three original predictors are included in the logistic regression model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#data\_f.glm$aic}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#data\_f.glm$xlevels}
\end{Highlighting}
\end{Shaded}

For getting a compact summary the function summary() is customized and formatted

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formatSummary }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model\_summary) \{}
\NormalTok{  aux\_coeff }\OtherTok{\textless{}{-}}\NormalTok{ model\_summary}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{]}
\NormalTok{  aux\_prob }\OtherTok{\textless{}{-}}\NormalTok{ model\_summary}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{4}\NormalTok{]}
\NormalTok{  aux\_stars }\OtherTok{\textless{}{-}} \FunctionTok{symnum}\NormalTok{(aux\_prob, }
                       \AttributeTok{corr =} \ConstantTok{FALSE}\NormalTok{, }
                       \AttributeTok{na =} \ConstantTok{FALSE}\NormalTok{,}
                       \AttributeTok{cutpoints =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                       \AttributeTok{symbols =} \FunctionTok{c}\NormalTok{(}\StringTok{"***"}\NormalTok{, }\StringTok{"**"}\NormalTok{, }\StringTok{"*"}\NormalTok{, }\StringTok{"."}\NormalTok{, }\StringTok{" "}\NormalTok{))}
  \FunctionTok{names}\NormalTok{(aux\_coeff) }\OtherTok{\textless{}{-}} \FunctionTok{str\_trunc}\NormalTok{(}\FunctionTok{names}\NormalTok{(aux\_coeff), }
                                \AttributeTok{width =} \DecValTok{40}\NormalTok{)}
\NormalTok{  aux\_result }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Estimate =}\NormalTok{ aux\_coeff, }
                           \AttributeTok{Prob\_z =}\NormalTok{ aux\_prob, }
                           \StringTok{"Stars"} \OtherTok{=}\NormalTok{ aux\_stars)}
  \FunctionTok{return}\NormalTok{(aux\_result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#summary(data\_f.glm) \%\textgreater{}\% formatSummary()}
\end{Highlighting}
\end{Shaded}

\newpage

\section{Building scorecard-models (scm) and calculating scorepoints}\label{building-scorecard-models-scm-and-calculating-scorepoints}

Scorepoints are calculated by combing scorecard-model, which combines
bin and glm information, with individual data

\subsection{Building scm-models: Combining bins.list \& data\_woe.glm in scorecard()}\label{building-scm-models-combining-bins.list-data_woe.glm-in-scorecard}

Building the scorecard via bin and glm information resulting from train
sample

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm }\OtherTok{\textless{}{-}}\NormalTok{ bins.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{scorecard}\NormalTok{(data\_woe.glm)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "basepoints"                          "status.of.existing.checking.account"
## [3] "duration.in.month"                   "credit.history"                     
## [5] "savings.account.and.bonds"           "purpose"
\end{verbatim}

Investigating the content of the ``woe-based'' scorecard model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{basepoints}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      variable    bin    woe points
##        <char> <lgcl> <lgcl>  <num>
## 1: basepoints     NA     NA    449
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{duration.in.month[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             variable       bin count count_distr   neg   pos   posprob
##               <char>    <char> <int>       <num> <int> <int>     <num>
## 1: duration.in.month  [-Inf,8)    63  0.08665750    57     6 0.0952381
## 2: duration.in.month    [8,16)   252  0.34662999   195    57 0.2261905
## 3: duration.in.month   [16,34)   281  0.38651994   191    90 0.3202847
## 4: duration.in.month   [34,44)    75  0.10316369    43    32 0.4266667
## 5: duration.in.month [44, Inf)    56  0.07702889    24    32 0.5714286
##           woe
##         <num>
## 1: -1.3967784
## 2: -0.3754349
## 3:  0.1020496
## 4:  0.5590492
## 5:  1.1421954
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{duration.in.month[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{9}\SpecialCharTok{:}\DecValTok{13}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             variable      bin_iv  total_iv breaks is_special_values points
##               <char>       <num>     <num> <char>            <lgcl>  <num>
## 1: duration.in.month 0.117489928 0.3165171      8             FALSE     97
## 2: duration.in.month 0.044932100 0.3165171     16             FALSE     26
## 3: duration.in.month 0.004106144 0.3165171     34             FALSE     -7
## 4: duration.in.month 0.035304912 0.3165171     44             FALSE    -39
## 5: duration.in.month 0.114683977 0.3165171    Inf             FALSE    -80
\end{verbatim}

\subsection{Calculating scorepoints: Combinig individual data.df \& scm-model in scorecard\_ply()}\label{calculating-scorepoints-combinig-individual-data.df-scm-model-in-scorecard_ply}

Generating a score list

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{scorecard\_ply}\NormalTok{(x, scorecard.scm)) }
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: The only\_total\_score=TRUE (= default argument) has to be used
for providing two compatible lists for further processing. If scores to
the different predictors are of interest, the two separate, i.e.~train
and validate samples have to analyzed individually with the argument
only\_total\_score=FALSE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "train"    "validate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   633
## 2:   351
## 3:   351
## 4:   477
## 5:   553
## 6:   625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   349
## 2:   531
## 3:   390
## 4:   287
## 5:   455
## 6:   408
\end{verbatim}

\newpage

\section{WoE-based predicting (forecasting) of probabilities and scorepoints}\label{woe-based-predicting-forecasting-of-probabilities-and-scorepoints}

\subsection{Predicting probabilities: Combining data\_woe.list \& data\_woe.glm in predict()}\label{predicting-probabilities-combining-data_woe.list-data_woe.glm-in-predict}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{predict}\NormalTok{(data\_woe.glm,}
                             \AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{,}
\NormalTok{                             x))}
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: Due to the fact that the data\_woe.glm was calibrated for the
train sample two different types of prediction can be destinguished,
i.e.~the in-sample (IS) prediction by using the train sample in the
predict()-function, and the out-of-sample (OoS) prediction by using the
test sample in the predict()-function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "train"    "validate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{() }\CommentTok{\# In{-}Sample prediction}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1          2          3          4          5          6 
## 0.03257030 0.62644835 0.62511261 0.22923725 0.09405324 0.03635829
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{() }\CommentTok{\# Out{-}of{-}Sample prediction}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1         2         3         4         5         6 
## 0.6293632 0.1222070 0.4915309 0.8008039 0.2815743 0.4315283
\end{verbatim}

\subsection{Predicting scorepoints: Retrieving predictions from score.list generated in scorecard\_ply()}\label{predicting-scorepoints-retrieving-predictions-from-score.list-generated-in-scorecard_ply}

The prediction of the scorepoints is alread incorported in the built
scorecard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   633
## 2:   351
## 3:   351
## 4:   477
## 5:   553
## 6:   625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   349
## 2:   531
## 3:   390
## 4:   287
## 5:   455
## 6:   408
\end{verbatim}

\newpage

\section{Scorecard Validation: Statistical testing of forecasting accuracy}\label{scorecard-validation-statistical-testing-of-forecasting-accuracy}

\subsection{Checking stability of score and probility distributions: Population Stability Index (PSI)}\label{checking-stability-of-score-and-probility-distributions-population-stability-index-psi}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list }\OtherTok{\textless{}{-}} \FunctionTok{perf\_psi}\NormalTok{(}\AttributeTok{score =}\NormalTok{ score.list, }
                     \AttributeTok{label =}\NormalTok{ default.list,}
                     \AttributeTok{return\_distr\_dat =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: More details of per\_psi() function are given @
\url{https://www.rdocumentation.org/packages/scorecard/versions/0.1.9/topics/perf_psi}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{pic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $score
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-41-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "pic" "psi" "dat"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "score"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{dat}\SpecialCharTok{$}\NormalTok{score[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Key: <datset>
##    datset        bin count cum_count   neg   pos cum_neg cum_pos count_distr
##    <fctr>     <fctr> <int>     <int> <int> <int>   <int>   <int>       <num>
## 1:  train [-Inf,260)    11        11     1    10       1      10      0.0151
## 2:  train  [260,314)    28        39     8    20       9      30      0.0385
## 3:  train  [314,368)    65       104    21    44      30      74      0.0894
## 4:  train  [368,421)   131       235    70    61     100     135      0.1802
## 5:  train  [421,475)   149       384   102    47     202     182      0.2050
## 6:  train  [475,529)   122       506    99    23     301     205      0.1678
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    variable        dataset        psi
##      <char>         <char>      <num>
## 1:    score train_validate 0.03247355
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{perf\_psi}\NormalTok{(score, }\AttributeTok{label =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{title =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{x\_limits =} \ConstantTok{NULL}\NormalTok{,}
  \AttributeTok{x\_tick\_break =} \DecValTok{50}\NormalTok{, }\AttributeTok{show\_plot =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{seed =} \DecValTok{186}\NormalTok{,}
  \AttributeTok{return\_distr\_dat =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# e.g. \# x\_limits = c(250, 700),}
\CommentTok{\#      \# x\_tick\_break = 50,}
\end{Highlighting}
\end{Shaded}

\subsection{IS \& OoS testing probability prediction accuracy: perf\_eva(.,predProb.list)}\label{is-oos-testing-probability-prediction-accuracy-perf_eva.predprob.list}

probability prediction accuracy

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy }\OtherTok{\textless{}{-}} \FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ predProb.list,}
                             \AttributeTok{label =}\NormalTok{ default.list,}
                             \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
                             \AttributeTok{show\_plot=}\FunctionTok{c}\NormalTok{(}\StringTok{"roc"}\NormalTok{,}\StringTok{"ks"}\NormalTok{),}
                             \AttributeTok{confusion\_matrix =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-44-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(ProbPredAccuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "binomial_metric"  "confusion_matrix" "pic"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy}\SpecialCharTok{$}\NormalTok{binomial\_metric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##         RMSE       AUC      Gini
##        <num>     <num>     <num>
## 1: 0.3998542 0.8018569 0.6037137
## 
## $validate
##         RMSE      AUC     Gini
##        <num>    <num>    <num>
## 1: 0.4150501 0.776506 0.553012
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy}\SpecialCharTok{$}\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    330    180 0.3529412
## 2:      1     41    176 0.1889401
## 3:  total    371    356 0.3039890
## 
## $validate
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    122     68 0.3578947
## 2:      1     17     66 0.2048193
## 3:  total    139    134 0.3113553
\end{verbatim}

Excursion

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ predProb.list, }
         \AttributeTok{label =}\NormalTok{ default.list,}
         \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
         \AttributeTok{show\_plot=} \ConstantTok{FALSE}\NormalTok{,}
         \AttributeTok{confusion\_matrix =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $binomial_metric
## $binomial_metric$train
##         RMSE       AUC      Gini
##        <num>     <num>     <num>
## 1: 0.3998542 0.8018569 0.6037137
## 
## $binomial_metric$validate
##         RMSE      AUC     Gini
##        <num>    <num>    <num>
## 1: 0.4150501 0.776506 0.553012
\end{verbatim}

\subsection{IS \& OoS testing scorepoint prediction accuracy: perf\_eva(.,score.list)}\label{is-oos-testing-scorepoint-prediction-accuracy-perf_eva.score.list}

scorepoint prediction accuracy

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy }\OtherTok{\textless{}{-}} \FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ score.list,}
                             \AttributeTok{label =}\NormalTok{ default.list,}
                             \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
                             \AttributeTok{show\_plot=}\FunctionTok{c}\NormalTok{(}\StringTok{"roc"}\NormalTok{,}\StringTok{"ks"}\NormalTok{),}
                             \AttributeTok{confusion\_matrix =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-49-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(ScorePredAccuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "binomial_metric"  "confusion_matrix" "pic"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy}\SpecialCharTok{$}\NormalTok{binomial\_metric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##          AUC      Gini
##        <num>     <num>
## 1: 0.8019563 0.6039125
## 
## $validate
##          AUC      Gini
##        <num>     <num>
## 1: 0.7759036 0.5518072
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy}\SpecialCharTok{$}\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    330    180 0.3529412
## 2:      1     41    176 0.1889401
## 3:  total    371    356 0.3039890
## 
## $validate
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    122     68 0.3578947
## 2:      1     17     66 0.2048193
## 3:  total    139    134 0.3113553
\end{verbatim}

\newpage

\section{Appendix}\label{appendix}

\subsection{Appendix: Essay style with formulas in LaTeX language}\label{appendix-essay-style-with-formulas-in-latex-language}

\textbf{Group project assignment}: Write a scholarly essay with full
sentences, correct citations and LaTeX formulas.

\textbf{Example essay style}: From a statistical perspective the transition
from the \(MPS\) to the VaR framework is related to switching the
perspective from considering moments (parameters) of random variables,
i.e.~\(\mu\) and \(\sigma\), to considering the quantiles and corresponding
probabilities of these variables. Specifically, the VaR measure
specifies the risk of a random variable (\(\tilde{P}\)) via the threshold
quantile (\(VaR\)) that is exceeded into the negative direction (i.e.
\(P \leq VaR\)) with the loss probability (\(\alpha\)) or respectively, is
exceeded into the positive direction (i.e.~\(P > VaR\)) with the
complementary probability, i.e.~the confidence level (\(1-\alpha\)).

\subsection{Appendix: Generating tables, figures, cross references and citations}\label{appendix-generating-tables-figures-cross-references-and-citations}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{Scorecard_Intro_2511_files/figure-latex/testFigure-1} 

}

\caption{Amount vs. Duration}\label{fig:testFigure}
\end{figure}

Formulas without numbering \begin{align*}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align*}

Formulas with numbering (and labeling which is needed for referencing)
\begin{align} \label{eq:VaR}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align}

Formula \eqref{eq:VaR} is a sample formula defining the Value at Risk.

Always cite original literature to avoid plagiarism: e.g.
\cite{SchwaigerIUF} or \citep{SchwaigerIUF}. Don't forget to cite page
numbers as well for literal citations, e.g.~\cite[p. 100]{SchwaigerIUF}.

\newpage

\bibliography{literature.bib}

\end{document}
