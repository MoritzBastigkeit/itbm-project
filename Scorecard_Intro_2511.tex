% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{titling}
\pretitle{\begin{center}\Large}
\posttitle{\par\end{center}}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdfauthor={Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\begin{center} Predictive Analytics: Application in the Credit Risk Domain \\ Case Study Teaching (CST)-Vignette in cheat sheet style \\ ("group project cover sheet") \end{center}}
\author{Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha}
\date{Nov.~27, 2025 (Scorecard\_Intro\_2511.Rmd)}

\begin{document}
\maketitle

\pagebreak

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\section{Abstract}\label{abstract}

Credit risk assessment is a central task in retail banking, ensuring that
financial institutions can effectively discriminate between creditworthy
and non-creditworthy applicants. Modern credit scoring increasingly
relies on data-driven methods that combine statistical modelling,
machine learning techniques and domain-specific transformations to
produce interpretable and stable credit-risk predictions.\\
This project applies a \textbf{Weight-of-Evidence (WoE)} and
\textbf{Information Value (IV)}--driven scorecard modelling framework using
the \emph{German Credit} dataset. The workflow includes data preparation,
IV-based feature assessment, supervised binning, WoE transformation,
logistic regression modelling, scorecard generation and out-of-sample
validation. Performance is evaluated through stability measures such as
the \textbf{Population Stability Index (PSI)} and accuracy metrics including
AUC, Gini and RMSE. The resulting scorecard provides a transparent,
regulator-compliant and empirically robust tool for credit-risk
prediction.

\section{Introduction}\label{introduction}

Credit risk modelling aims to quantify the likelihood that a borrower
will fail to meet contractual repayment obligations. As banks, insurers
and financial intermediaries increasingly rely on data-driven decision
frameworks, scorecards have become the industry standard for credit
underwriting due to their transparency, interpretability and regulatory
acceptance \citep{HandHenley1997, Thomas2002}. In contrast to purely
algorithmic black-box models, scorecards preserve a clear link between
economic reasoning, data transformations and model parameters---an aspect
that is essential for auditability and explainability in regulated
environments.

comment: my introduction, might merge it with this
A central function of banks is to provide loans to individuals and companies. Credit scoring models are an essential tool for banks to assess the creditworthiness of lenders and predict how likely they are to meet their financial obligations. Popular models are based on the 3C's, 4C's, or 5C's, which stand for character, capital, collateral, capacity, and condition. However, with advancing technology, more novel approaches have emerged. In the subsequent paper, a credit scoring model is built and evaluated based on German data. The objective of the paper is to investigate the predictive value of demographic attributes. The objective of this study is not to enhance accuracy, but rather to offer insight into the structure of creditworthiness within the German context.

The aim of this project is to build and validate a \textbf{predictive
scorecard model} based on the \emph{German Credit} dataset.
To achieve this, the project adopts a structured analytical workflow
aligned with established credit-risk modelling guidelines
\citep{Anderson2007, Siddiqi2017}. The key methodological components are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data Filtering and Variable Selection} -- Choosing a
  subset of variables that capture behavioural and financial
  characteristics of borrowers.
\item
  \textbf{Information Value Analysis (IV)} -- Quantifying the predictive strength
  of candidate variables.
\item
  \textbf{Supervised Binning and Weight-of-Evidence Transformation (WoE)} --
  Creating monotonic predictor--default relationships and ensuring stable
  logistic-regression estimation.
\item
  \textbf{Logistic Regression Modelling} -- Fitting a parsimonious and
  interpretable model linking WoE-transformed predictors to the
  probability of default.
\item
  \textbf{Scorecard Generation} -- Translating the regression coefficients and
  bin structures into a practical scorecard with additive scorepoints.
\item
  \textbf{Model Validation} -- Assessing predictive accuracy (AUC, Gini, RMSE),
  calibration, and population stability (PSI) for both in-sample (IS)
  and out-of-sample (OoS) samples.
\end{enumerate}

Using WoE transformations is beneficial because it enforces
monotonicity, reduces the influence of outliers and yields logistic
models with minimal multicollinearity \citep{Siddiqi2017}. This
ensures that the final scorecard is both empirically robust.

Overall, this project demonstrates how predictive analytics can be
applied to credit-risk modelling to produce a validated scorecard.
The methodological steps should lead to a replicable blueprint for building
credit-risk models.

\section{Methodology}\label{methodology}

In order to build and assess these models, the Weight-of-Evidence (WoE) approach was taken. In it, the raw features of individuals are transformed in that they are sorted in bins based on their predictive strength to distinguish between defaulters and non-defaulters. To calculate it, the logarithm of the ratio of non-defaulters to defaulters within a group is taken, resulting in a score that represents credit risk. In doing so, the WoE approach allows to convert categorical or numerical values into values that are then processed in a logistic regression or scorecard model. In the underlying general formula, the event is defined in the underlying case as being a \textit{good customer}, meaning that this individual repays their debt towards the bank, respectively in each group \(i\).
\begin{equation}
\label{woe_eq}
WoE_i = \ln\left(\frac{Event_i\%}{NonEvent_i\%}\right)
\end{equation}
The resulting predictor variables were then used in two models, first a logistic regression and secondly a scorecard model, to compare the accuracy of each. Both models aim to predict the probability of default \(P(Y=1)\) for the given set of predictors with the logistic approach using a sigmoid function. Sigmoid functions are mathematical functions, usually shaped like an S, used for classification problems by taking real numbers as an input and transforming them to values between 0 and 1 (probabilities). The model takes the form\[P(default=1|X) = \frac{1}{1+e^{-(\beta_0+\beta_1X_1+...+\beta_kX_k)}}\] where the coefficients \(\beta\) represent the individual contribution of each variable to the likelihood of default.

On the other hand, a scorecard model was implemented to serve as a more transparent comparison to the logistic regression. After the variables are initially binned and converted into their respective WoE values, each bin of each variable was assigned a fixed number of score points that reflect its contribution to the credit risk. This was done by using the \(scorecard()\) function from the eponymous package in R. For any individual applicant, the total credit score was then calculated by summing up the score points across their variables with higher scores representing lower risk of defaulting.

To evaluate and the performance of these models, a selected number of accuracy metrics, namely Area Under the ROC Curve (AUC) and the Gini coefficient, was used. These metrics measure how well the model is able to differentiate between defaulters and non-defaulters. Furthermore, the Root Mean Squared Error (RSME) quantifies the models' accuracy in their predicted probabilities by quantifying the average deviation between predicted and actual outcomes (in the train set). Additionally, matrices summarizing the performance by reporting true positives, false positives, true negatives and false negatives were generated (confusion matrices). The stability or robustness of the models' scores and probability distributions were assessed with the Population Stability Index (PSI). This metric compares the distributions in the training set with those of the validation set to detect shifts in population characteristics across different samples. In this context, a low score is indicative of a stable model whereas high values suggest a potential drift or deterioration.

\section{Data Selection}\label{data-selection}

To train the models to accurately predict the creditworthiness, it is imperative to select predictor variables based on their relevance to the individuals' financial behaviour.

To get a better understanding of the data structure, Table 1 gives an insight into the German dataset.

\begin{table}[!h]
\centering
\caption{\label{tab:dataTable}Exemplary View of the Data Table}
\centering
\begin{tabular}[t]{lcc}
\toprule
creditability & status.of.existing.checking.account & duration.in.month\\
\midrule
good & ... < 0 DM & 6\\
bad & 0 <= ... < 200 DM & 48\\
good & no checking account & 12\\
good & ... < 0 DM & 42\\
bad & ... < 0 DM & 24\\
\addlinespace
good & no checking account & 36\\
\bottomrule
\end{tabular}
\end{table}

The selection process focused on variables that reflect past repayment behaviour, financial stability, and loan characteristics. Restricting the model to these input variables aims to produce transparent and robust results that reflect the real world risk modeling practices. To do so, each variable from the German dataset was evaluated on its Information Value (IV) to determine how well they are suited in discriminating between good and bad borrowers (data quality). Variables with high scores were retained for the subsequent modeling and further investigated in their underlying structuring. In practical terms, a higher IV relates to stronger predictive power of the underlying variable, with general thresholds of being good predictors of values larger than 0.3. Table 2 visualizes the calculated IV scores for the selected variables.

The five selected predictor variables are:

\begin{itemize}

\item \textbf{status.of.existing.checking.account} (categorical) \\
This is a categorical variable that describes the applicant's checking account condition using qualitative labels such as "no checking account", "<0 DM", etc. 

\item \textbf{duration.in.month} (numeric) \\
This is a numeric variable indicating the length of the loan contract in months, reflecting how long the applicant will take to repay off the credit.

\item \textbf{credit.history} (categorical) \\
This is another categorical variable that describes the applicant's past repayment behaviour, ranging from values of "no credits taken" and "all credits in this bank paid back duly" to "critical account".

\item \textbf{savings.account.and.bonds} (categorical) \\
This is another variable that categorises the applicant's savings level, both in their savings account and bonds. There are several categories such as "unknown/no savings account" to " < 100 DM" to separate those into groups.

\item \textbf{purpose} (categorical) \\
Purpose reflects a categorical variable that gives insight into what the applicant intends to do with the credit. Values range from "used car" and "new car" to "education", etc. 
\end{itemize}

\begin{table}[!h]
\centering
\caption{\label{tab:dataIVTable}Information Value of Variables}
\centering
\begin{tabular}[t]{lc}
\toprule
Variable & Information Value\\
\midrule
\cellcolor{gray!10}{status.of.existing.checking.account} & \cellcolor{gray!10}{0.67}\\
duration.in.month & 0.33\\
\cellcolor{gray!10}{credit.history} & \cellcolor{gray!10}{0.29}\\
savings.account.and.bonds & 0.20\\
\cellcolor{gray!10}{purpose} & \cellcolor{gray!10}{0.17}\\
\bottomrule
\end{tabular}
\end{table}

\begin{verbatim}
## v Variable filtering on 1000 rows and 5 columns in 00:00:00
## v 0 variables are removed in total
\end{verbatim}

In the following steps, the data was filtered to exclude rows with missing values and split into train and validation set. The split chosen was at a ratio of .75 to .25. This is being done to have two independent samples for training and validating the model at a later point.

\section{Weight-Of-Evidence (WoE)-based transformation of predictor variables}\label{weight-of-evidence-woe-based-transformation-of-predictor-variables}

\subsection{WoE-based binning of train and validate samples: bins.list}\label{woe-based-binning-of-train-and-validate-samples-bins.list}

WoE-based classing, i.e.~binning and grouping of predictor variables

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## v Binning on 635 rows and 6 columns in 00:00:00
\end{verbatim}

\textbf{Hint}: The default binning method is method=``width''. Other methods are

\begin{itemize}
\item
  ``frequ'' that support numerical variables as well as
\item
  ``tree'' and ``chimerge'' supporting both, i.e.~numerical and
  categorical variables which are used in the optimal binning
  approach.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "status.of.existing.checking.account" "duration.in.month"                  
## [3] "credit.history"                      "savings.account.and.bonds"          
## [5] "purpose"
\end{verbatim}

Plotting the bins (including bin statistics)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{status.of.existing.checking.account }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $status.of.existing.checking.account
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-4-1} \end{center}

\textbf{Hint}: credit.amount does not have an acceptable structure of the default rates (positive probability) over the bins like e.g.~a linear or u-curve structure; hence it should not be included in the scorecard model!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{duration.in.month }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $duration.in.month
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{credit.history }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $credit.history
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{savings.account.and.bonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $savings.account.and.bonds
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list}\SpecialCharTok{$}\NormalTok{purpose }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin\_plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $purpose
\end{verbatim}

\begin{center}\includegraphics{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-8-1} \end{center}

\textbf{Hint}: duration.in.month has lineare structure of the default rates; hence it should be included in the scorecard model!

\textbf{Excursion}: Manual bin-adjustments

Bins can be altered manually by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Saving the bin list generated in the woebin() function via e.g.
  save\_as=``breaks2410.list''
\item
  Loading the saved R-file ``breaks2410.list.R'', editing the breaks as
  needed and storing the file
\item
  Sourcing the edited and stored ``breaks2410.list.R'' file with the
  ``source(\ldots)\$value'' function
\item
  Binning the data again with the ``woebin()'' function with the
  additional argument ``break\_list''
\end{enumerate}

ad 1)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{,}
         \AttributeTok{save\_as =} \StringTok{"breaks2410.list"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

ad 3)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{breaksList }\OtherTok{\textless{}{-}} \FunctionTok{source}\NormalTok{(}\StringTok{"breaks2410.list.R"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{value}
\end{Highlighting}
\end{Shaded}

ad 4)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bins.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{woebin}\NormalTok{(}\StringTok{"creditability"}\NormalTok{,}
         \AttributeTok{breaks\_list =} \StringTok{"breaksList"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: The above code chunks are not yet evaluated, as they are
performed only when the original binning does not deliver beneficial
results.

\subsection{WoE-based transforming of predictor variables: data\_woe.list}\label{woe-based-transforming-of-predictor-variables-data_woe.list}

\subsubsection{WoE-based transforming of train and validate data: data\_woe.list}\label{woe-based-transforming-of-train-and-validate-data-data_woe.list}

Transforming splitted sample: Needed for train/validate analysis

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{woebin\_ply}\NormalTok{(x, bins.list))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## v Woe transformating on 635 rows and 5 columns in 00:00:00
\end{verbatim}

\begin{verbatim}
## v Woe transformating on 365 rows and 5 columns in 00:00:10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{lapply}\NormalTok{(class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
## [1] "data.table" "data.frame"
## 
## $validate
## [1] "data.table" "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{lapply}\NormalTok{(dim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
## [1] 635   6
## 
## $validate
## [1] 365   6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{woebin\_ply}\NormalTok{(x, bins.list))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## v Woe transformating on 635 rows and 5 columns in 00:00:00
\end{verbatim}

\begin{verbatim}
## v Woe transformating on 365 rows and 5 columns in 00:00:10
\end{verbatim}

\newpage

\section{Generalized linear model (glm): Regressing predictors against responses}\label{generalized-linear-model-glm-regressing-predictors-against-responses}

\subsection{Logistic regression of WoE-transformed predictors: glm(.,data\_woe.list\$train)}\label{logistic-regression-of-woe-transformed-predictors-glm.data_woe.listtrain}

The WoE-based logistic regression is the preferred regression approach
as it delivers the most compact regression models.

\subsubsection{Constructing and calibrating the WoE-based logistic regression model}\label{constructing-and-calibrating-the-woe-based-logistic-regression-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
\NormalTok{  creditability }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(),}
  \AttributeTok{data =}\NormalTok{ data\_woe.list}\SpecialCharTok{$}\NormalTok{train}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Investigating the fitted regression model}\label{investigating-the-fitted-regression-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_woe.glm}\SpecialCharTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 619.9392
\end{verbatim}

Summary of regression: summary()

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(data\_woe.glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = creditability ~ ., family = binomial(), data = data_woe.list$train)
## 
## Coefficients:
##                                         Estimate Std. Error z value Pr(>|z|)
## (Intercept)                              -0.8628     0.1023  -8.435  < 2e-16
## status.of.existing.checking.account_woe   0.8195     0.1310   6.254 3.99e-10
## duration.in.month_woe                     0.9772     0.1897   5.152 2.58e-07
## credit.history_woe                        0.7643     0.1754   4.357 1.32e-05
## savings.account.and.bonds_woe             0.8944     0.2697   3.316 0.000912
## purpose_woe                               0.9840     0.2506   3.927 8.59e-05
##                                            
## (Intercept)                             ***
## status.of.existing.checking.account_woe ***
## duration.in.month_woe                   ***
## credit.history_woe                      ***
## savings.account.and.bonds_woe           ***
## purpose_woe                             ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 769.77  on 634  degrees of freedom
## Residual deviance: 607.94  on 629  degrees of freedom
## AIC: 619.94
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

\subsection{Logistic regression of original predictors: glm(.,data\_f.list\$train)}\label{logistic-regression-of-original-predictors-glm.data_f.listtrain}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_f.glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
\NormalTok{  creditability }\SpecialCharTok{\textasciitilde{}}\NormalTok{ status.of.existing.checking.account }\SpecialCharTok{+}\NormalTok{ duration.in.month }\SpecialCharTok{+}\NormalTok{ credit.history,}
  \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(),}
  \AttributeTok{data =}\NormalTok{ data\_f.list}\SpecialCharTok{$}\NormalTok{train}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: For simplicity only three original predictors are included in the logistic regression model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_f.glm}\SpecialCharTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 656.1663
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_f.glm}\SpecialCharTok{$}\NormalTok{xlevels}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $status.of.existing.checking.account
## [1] "... < 0 DM"                                            
## [2] "0 <= ... < 200 DM"                                     
## [3] "... >= 200 DM / salary assignments for at least 1 year"
## [4] "no checking account"                                   
## 
## $credit.history
## [1] "no credits taken/ all credits paid back duly"               
## [2] "all credits at this bank paid back duly"                    
## [3] "existing credits paid back duly till now"                   
## [4] "delay in paying off in the past"                            
## [5] "critical account/ other credits existing (not at this bank)"
\end{verbatim}

For getting a compact summary the function summary() is customized and formatted

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formatSummary }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model\_summary) \{}
\NormalTok{  aux\_coeff }\OtherTok{\textless{}{-}}\NormalTok{ model\_summary}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{]}
\NormalTok{  aux\_prob }\OtherTok{\textless{}{-}}\NormalTok{ model\_summary}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{4}\NormalTok{]}
\NormalTok{  aux\_stars }\OtherTok{\textless{}{-}} \FunctionTok{symnum}\NormalTok{(aux\_prob, }
                       \AttributeTok{corr =} \ConstantTok{FALSE}\NormalTok{, }
                       \AttributeTok{na =} \ConstantTok{FALSE}\NormalTok{,}
                       \AttributeTok{cutpoints =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                       \AttributeTok{symbols =} \FunctionTok{c}\NormalTok{(}\StringTok{"***"}\NormalTok{, }\StringTok{"**"}\NormalTok{, }\StringTok{"*"}\NormalTok{, }\StringTok{"."}\NormalTok{, }\StringTok{" "}\NormalTok{))}
  \FunctionTok{names}\NormalTok{(aux\_coeff) }\OtherTok{\textless{}{-}} \FunctionTok{str\_trunc}\NormalTok{(}\FunctionTok{names}\NormalTok{(aux\_coeff), }
                                \AttributeTok{width =} \DecValTok{40}\NormalTok{)}
\NormalTok{  aux\_result }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Estimate =}\NormalTok{ aux\_coeff, }
                           \AttributeTok{Prob\_z =}\NormalTok{ aux\_prob, }
                           \StringTok{"Stars"} \OtherTok{=}\NormalTok{ aux\_stars)}
  \FunctionTok{return}\NormalTok{(aux\_result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(data\_f.glm) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{formatSummary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                             Estimate       Prob_z Stars
## (Intercept)                               0.36752107 4.879382e-01      
## status.of.existing.checking.account0 ... -0.36279765 1.142842e-01      
## status.of.existing.checking.account..... -1.25349211 6.084361e-03    **
## status.of.existing.checking.accountno... -1.85705728 3.799756e-12   ***
## duration.in.month                         0.03353515 9.890970e-06   ***
## credit.historyall credits at this ban... -0.54363263 3.753992e-01      
## credit.historyexisting credits paid b... -1.18265150 1.571509e-02     *
## credit.historydelay in paying off in ... -1.00684149 7.091344e-02     .
## credit.historycritical account/ other... -1.91787010 2.306172e-04   ***
\end{verbatim}

\newpage

\section{Building scorecard-models (scm) and calculating scorepoints}\label{building-scorecard-models-scm-and-calculating-scorepoints}

Scorepoints are calculated by combing scorecard-model, which combines
bin and glm information, with individual data

\subsection{Building scm-models: Combining bins.list \& data\_woe.glm in scorecard()}\label{building-scm-models-combining-bins.list-data_woe.glm-in-scorecard}

Building the scorecard via bin and glm information resulting from train
sample

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm }\OtherTok{\textless{}{-}}\NormalTok{ bins.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{scorecard}\NormalTok{(data\_woe.glm)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "basepoints"                          "status.of.existing.checking.account"
## [3] "duration.in.month"                   "credit.history"                     
## [5] "savings.account.and.bonds"           "purpose"
\end{verbatim}

Investigating the content of the ``woe-based'' scorecard model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{basepoints}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      variable    bin    woe points
##        <char> <lgcl> <lgcl>  <num>
## 1: basepoints     NA     NA    450
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{duration.in.month[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             variable       bin count count_distr   neg   pos   posprob
##               <char>    <char> <int>       <num> <int> <int>     <num>
## 1: duration.in.month  [-Inf,8)    56  0.08818898    50     6 0.1071429
## 2: duration.in.month    [8,16)   214  0.33700787   166    48 0.2242991
## 3: duration.in.month   [16,26)   204  0.32125984   145    59 0.2892157
## 4: duration.in.month   [26,44)   108  0.17007874    64    44 0.4074074
## 5: duration.in.month [44, Inf)    53  0.08346457    23    30 0.5660377
##            woe
##          <num>
## 1: -1.24657892
## 2: -0.36710216
## 3: -0.02551168
## 4:  0.49899117
## 5:  1.13938778
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scorecard.scm}\SpecialCharTok{$}\NormalTok{duration.in.month[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{9}\SpecialCharTok{:}\DecValTok{13}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             variable       bin_iv  total_iv breaks is_special_values points
##               <char>        <num>     <num> <char>            <lgcl>  <num>
## 1: duration.in.month 0.0991299271 0.3115523      8             FALSE     88
## 2: duration.in.month 0.0417950298 0.3115523     16             FALSE     26
## 3: duration.in.month 0.0002079889 0.3115523     26             FALSE      2
## 4: duration.in.month 0.0461252338 0.3115523     44             FALSE    -35
## 5: duration.in.month 0.1242941288 0.3115523    Inf             FALSE    -80
\end{verbatim}

\subsection{Calculating scorepoints: Combinig individual data.df \& scm-model in scorecard\_ply()}\label{calculating-scorepoints-combinig-individual-data.df-scm-model-in-scorecard_ply}

Generating a score list

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_f.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{scorecard\_ply}\NormalTok{(x, scorecard.scm)) }
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: The only\_total\_score=TRUE (= default argument) has to be used
for providing two compatible lists for further processing. If scores to
the different predictors are of interest, the two separate, i.e.~train
and validate samples have to analyzed individually with the argument
only\_total\_score=FALSE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "train"    "validate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   630
## 2:   354
## 3:   357
## 4:   496
## 5:   557
## 6:   622
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   350
## 2:   551
## 3:   395
## 4:   285
## 5:   456
## 6:   420
\end{verbatim}

\newpage

\section{WoE-based predicting (forecasting) of probabilities and scorepoints}\label{woe-based-predicting-forecasting-of-probabilities-and-scorepoints}

\subsection{Predicting probabilities: Combining data\_woe.list \& data\_woe.glm in predict()}\label{predicting-probabilities-combining-data_woe.list-data_woe.glm-in-predict}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list }\OtherTok{\textless{}{-}}\NormalTok{ data\_woe.list }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{predict}\NormalTok{(data\_woe.glm,}
                             \AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{,}
\NormalTok{                             x))}
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: Due to the fact that the data\_woe.glm was calibrated for the
train sample two different types of prediction can be destinguished,
i.e.~the in-sample (IS) prediction by using the train sample in the
predict()-function, and the out-of-sample (OoS) prediction by using the
test sample in the predict()-function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "train"    "validate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{() }\CommentTok{\# In{-}Sample prediction}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1          2          3          4          5          6 
## 0.03390496 0.61729222 0.60866854 0.18293009 0.08764694 0.03756245
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predProb.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{() }\CommentTok{\# Out{-}of{-}Sample prediction}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1         2         3         4         5         6 
## 0.6311053 0.0952773 0.4777985 0.8080544 0.2818604 0.3935574
\end{verbatim}

\subsection{Predicting scorepoints: Retrieving predictions from score.list generated in scorecard\_ply()}\label{predicting-scorepoints-retrieving-predictions-from-score.list-generated-in-scorecard_ply}

The prediction of the scorepoints is alread incorported in the built
scorecard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   630
## 2:   354
## 3:   357
## 4:   496
## 5:   557
## 6:   622
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{score.list}\SpecialCharTok{$}\NormalTok{validate }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    score
##    <num>
## 1:   350
## 2:   551
## 3:   395
## 4:   285
## 5:   456
## 6:   420
\end{verbatim}

\newpage

\section{Scorecard Validation: Statistical testing of forecasting accuracy}\label{scorecard-validation-statistical-testing-of-forecasting-accuracy}

\subsection{Checking stability of score and probility distributions: Population Stability Index (PSI)}\label{checking-stability-of-score-and-probility-distributions-population-stability-index-psi}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list }\OtherTok{\textless{}{-}} \FunctionTok{perf\_psi}\NormalTok{(}\AttributeTok{score =}\NormalTok{ score.list, }
                     \AttributeTok{label =}\NormalTok{ default.list,}
                     \AttributeTok{return\_distr\_dat =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Hint}: More details of per\_psi() function are given @
\url{https://www.rdocumentation.org/packages/scorecard/versions/0.1.9/topics/perf_psi}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{pic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $score
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-39-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "pic" "psi" "dat"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "score"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{dat}\SpecialCharTok{$}\NormalTok{score[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Key: <datset>
##    datset        bin count cum_count   neg   pos cum_neg cum_pos count_distr
##    <fctr>     <fctr> <int>     <int> <int> <int>   <int>   <int>       <num>
## 1:  train [-Inf,271)    12        12     2    10       2      10      0.0189
## 2:  train  [271,323)    20        32     5    15       7      25      0.0315
## 3:  train  [323,374)    61        93    20    41      27      66      0.0961
## 4:  train  [374,426)   114       207    64    50      91     116      0.1795
## 5:  train  [426,478)   133       340    92    41     183     157      0.2094
## 6:  train  [478,530)   103       443    85    18     268     175      0.1622
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi.list}\SpecialCharTok{$}\NormalTok{psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    variable        dataset        psi
##      <char>         <char>      <num>
## 1:    score train_validate 0.04239616
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{perf\_psi}\NormalTok{(score, }\AttributeTok{label =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{title =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{x\_limits =} \ConstantTok{NULL}\NormalTok{,}
  \AttributeTok{x\_tick\_break =} \DecValTok{50}\NormalTok{, }\AttributeTok{show\_plot =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{seed =} \DecValTok{186}\NormalTok{,}
  \AttributeTok{return\_distr\_dat =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# e.g. \# x\_limits = c(250, 700),}
\CommentTok{\#      \# x\_tick\_break = 50,}
\end{Highlighting}
\end{Shaded}

\subsection{IS \& OoS testing probability prediction accuracy: perf\_eva(.,predProb.list)}\label{is-oos-testing-probability-prediction-accuracy-perf_eva.predprob.list}

probability prediction accuracy

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy }\OtherTok{\textless{}{-}} \FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ predProb.list,}
                             \AttributeTok{label =}\NormalTok{ default.list,}
                             \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
                             \AttributeTok{show\_plot=}\FunctionTok{c}\NormalTok{(}\StringTok{"roc"}\NormalTok{,}\StringTok{"ks"}\NormalTok{),}
                             \AttributeTok{confusion\_matrix =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-42-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(ProbPredAccuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "binomial_metric"  "confusion_matrix" "pic"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy}\SpecialCharTok{$}\NormalTok{binomial\_metric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##         RMSE       AUC      Gini
##        <num>     <num>     <num>
## 1: 0.3987765 0.8022465 0.6044929
## 
## $validate
##         RMSE       AUC     Gini
##        <num>     <num>    <num>
## 1: 0.4153243 0.7773915 0.554783
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ProbPredAccuracy}\SpecialCharTok{$}\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    289    159 0.3549107
## 2:      1     35    152 0.1871658
## 3:  total    324    311 0.3055118
## 
## $validate
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    168     84 0.3333333
## 2:      1     25     88 0.2212389
## 3:  total    193    172 0.2986301
\end{verbatim}

Excursion

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ predProb.list, }
         \AttributeTok{label =}\NormalTok{ default.list,}
         \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
         \AttributeTok{show\_plot=} \ConstantTok{FALSE}\NormalTok{,}
         \AttributeTok{confusion\_matrix =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $binomial_metric
## $binomial_metric$train
##         RMSE       AUC      Gini
##        <num>     <num>     <num>
## 1: 0.3987765 0.8022465 0.6044929
## 
## $binomial_metric$validate
##         RMSE       AUC     Gini
##        <num>     <num>    <num>
## 1: 0.4153243 0.7773915 0.554783
\end{verbatim}

\subsection{IS \& OoS testing scorepoint prediction accuracy: perf\_eva(.,score.list)}\label{is-oos-testing-scorepoint-prediction-accuracy-perf_eva.score.list}

scorepoint prediction accuracy

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy }\OtherTok{\textless{}{-}} \FunctionTok{perf\_eva}\NormalTok{(}\AttributeTok{pred =}\NormalTok{ score.list,}
                             \AttributeTok{label =}\NormalTok{ default.list,}
                             \AttributeTok{binomial\_metric =} \FunctionTok{c}\NormalTok{(}\StringTok{"rmse"}\NormalTok{,}\StringTok{"auc"}\NormalTok{,}\StringTok{"gini"}\NormalTok{),}
                             \AttributeTok{show\_plot=}\FunctionTok{c}\NormalTok{(}\StringTok{"roc"}\NormalTok{,}\StringTok{"ks"}\NormalTok{),}
                             \AttributeTok{confusion\_matrix =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{Scorecard_Intro_2511_files/figure-latex/unnamed-chunk-47-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(ScorePredAccuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "binomial_metric"  "confusion_matrix" "pic"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy}\SpecialCharTok{$}\NormalTok{binomial\_metric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##         AUC      Gini
##       <num>     <num>
## 1: 0.801966 0.6039319
## 
## $validate
##          AUC      Gini
##        <num>     <num>
## 1: 0.7779007 0.5558014
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ScorePredAccuracy}\SpecialCharTok{$}\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $train
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    289    159 0.3549107
## 2:      1     35    152 0.1871658
## 3:  total    324    311 0.3055118
## 
## $validate
##     label pred_0 pred_1     error
##    <char>  <num>  <num>     <num>
## 1:      0    168     84 0.3333333
## 2:      1     25     88 0.2212389
## 3:  total    193    172 0.2986301
\end{verbatim}

\newpage

\section{Appendix}\label{appendix}

\subsection{Appendix: Essay style with formulas in LaTeX language}\label{appendix-essay-style-with-formulas-in-latex-language}

\textbf{Group project assignment}: Write a scholarly essay with full
sentences, correct citations and LaTeX formulas.

\textbf{Example essay style}: From a statistical perspective the transition
from the \(MPS\) to the VaR framework is related to switching the
perspective from considering moments (parameters) of random variables,
i.e.~\(\mu\) and \(\sigma\), to considering the quantiles and corresponding
probabilities of these variables. Specifically, the VaR measure
specifies the risk of a random variable (\(\tilde{P}\)) via the threshold
quantile (\(VaR\)) that is exceeded into the negative direction (i.e.
\(P \leq VaR\)) with the loss probability (\(\alpha\)) or respectively, is
exceeded into the positive direction (i.e.~\(P > VaR\)) with the
complementary probability, i.e.~the confidence level (\(1-\alpha\)).

\subsection{Appendix: Generating tables, figures, cross references and citations}\label{appendix-generating-tables-figures-cross-references-and-citations}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{Scorecard_Intro_2511_files/figure-latex/testFigure-1} 

}

\caption{Amount vs. Duration}\label{fig:testFigure}
\end{figure}

Formulas without numbering \begin{align*}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align*}

Formulas with numbering (and labeling which is needed for referencing)
\begin{align} \label{eq:VaR}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align}

Formula \eqref{eq:VaR} is a sample formula defining the Value at Risk.

Always cite original literature to avoid plagiarism: e.g.
\cite{SchwaigerIUF} or \citep{SchwaigerIUF}. Don't forget to cite page
numbers as well for literal citations, e.g.~\cite[p. 100]{SchwaigerIUF}.

\newpage

\bibliography{literature.bib}

\end{document}
