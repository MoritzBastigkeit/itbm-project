% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{titling}
\pretitle{\begin{center}\Large}
\posttitle{\par\end{center}}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdfauthor={Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\begin{center} Predictive Analytics: Application in the Credit Risk Domain \\ Case Study Teaching (CST)-Vignette in cheat sheet style \\ ("group project cover sheet") \end{center}}
\author{Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha}
\date{Nov.~27, 2025 (Scorecard\_Intro\_2511.Rmd)}

\begin{document}
\maketitle

\pagebreak

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\section{Abstract}\label{abstract}

Credit risk assessment is a central task in retail banking, ensuring that
financial institutions can effectively discriminate between creditworthy
and non-creditworthy applicants. Modern credit scoring increasingly
relies on data-driven methods that combine statistical modelling,
machine learning techniques and domain-specific transformations to
produce interpretable and stable credit-risk predictions.\\
This project applies a \textbf{Weight-of-Evidence (WoE)} and
\textbf{Information Value (IV)}--driven scorecard modelling framework using
the \emph{German Credit} dataset. The workflow includes data preparation,
IV-based feature assessment, supervised binning, WoE transformation,
logistic regression modelling, scorecard generation and out-of-sample
validation. Performance is evaluated through stability measures such as
the \textbf{Population Stability Index (PSI)} and accuracy metrics including
AUC, Gini and RMSE. The resulting scorecard provides a transparent,
regulator-compliant and empirically robust tool for credit-risk
prediction.

\section{Introduction}\label{introduction}

Credit risk modelling aims to quantify the likelihood that a borrower
will fail to meet contractual repayment obligations. As banks, insurers
and financial intermediaries increasingly rely on data-driven decision
frameworks, scorecards have become the industry standard for credit
underwriting due to their transparency, interpretability and regulatory
acceptance \citep{HandHenley1997, Thomas2002}. In contrast to purely
algorithmic black-box models, scorecards preserve a clear link between
economic reasoning, data transformations and model parameters---an aspect
that is essential for auditability and explainability in regulated
environments.

comment: my introduction, might merge it with this
A central function of banks is to provide loans to individuals and companies. Credit scoring models are an essential tool for banks to assess the creditworthiness of lenders and predict how likely they are to meet their financial obligations. Popular models are based on the 3C's, 4C's, or 5C's, which stand for character, capital, collateral, capacity, and condition. However, with advancing technology, more novel approaches have emerged. In the subsequent paper, a credit scoring model is built and evaluated based on German data. The objective of the paper is to investigate the predictive value of demographic attributes. The objective of this study is not to enhance accuracy, but rather to offer insight into the structure of creditworthiness within the German context.

The aim of this project is to build and validate a \textbf{predictive
scorecard model} based on the \emph{German Credit} dataset.
To achieve this, the project adopts a structured analytical workflow
aligned with established credit-risk modelling guidelines
\citep{Anderson2007, Siddiqi2017}. The key methodological components are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data Filtering and Variable Selection} -- Choosing a
  subset of variables that capture behavioural and financial
  characteristics of borrowers.
\item
  \textbf{Information Value Analysis (IV)} -- Quantifying the predictive strength
  of candidate variables.
\item
  \textbf{Supervised Binning and Weight-of-Evidence Transformation (WoE)} --
  Creating monotonic predictor--default relationships and ensuring stable
  logistic-regression estimation.
\item
  \textbf{Logistic Regression Modelling} -- Fitting a parsimonious and
  interpretable model linking WoE-transformed predictors to the
  probability of default.
\item
  \textbf{Scorecard Generation} -- Translating the regression coefficients and
  bin structures into a practical scorecard with additive scorepoints.
\item
  \textbf{Model Validation} -- Assessing predictive accuracy (AUC, Gini, RMSE),
  calibration, and population stability (PSI) for both in-sample (IS)
  and out-of-sample (OoS) samples.
\end{enumerate}

Using WoE transformations is beneficial because it enforces
monotonicity, reduces the influence of outliers and yields logistic
models with minimal multicollinearity \citep{Siddiqi2017}. This
ensures that the final scorecard is both empirically robust.

Overall, this project demonstrates how predictive analytics can be
applied to credit-risk modelling to produce a validated scorecard.
The methodological steps should lead to a replicable blueprint for building
credit-risk models.

\section{Methodology}\label{methodology}

In order to build and assess these models, the Weight-of-Evidence (WoE) approach was taken. In it, the raw features of individuals are transformed in that they are sorted in bins based on their predictive strength to distinguish between defaulters and non-defaulters. To calculate it, the logarithm of the ratio of non-defaulters to defaulters within a group is taken, resulting in a score that represents credit risk. In doing so, the WoE approach allows to convert categorical or numerical values into values that are then processed in a logistic regression or scorecard model. In the underlying general formula, the event is defined in the underlying case as being a \textit{good customer}, meaning that this individual repays their debt towards the bank, respectively in each group \(i\).
\begin{equation}
\label{woe_eq}
WoE_i = \ln\left(\frac{Event_i\%}{NonEvent_i\%}\right)
\end{equation}
The resulting predictor variables were then used in two models, first a logistic regression and secondly a scorecard model, to compare the accuracy of each. Both models aim to predict the probability of default \(P(Y=1)\) for the given set of predictors with the logistic approach using a sigmoid function. Sigmoid functions are mathematical functions, usually shaped like an S, used for classification problems by taking real numbers as an input and transforming them to values between 0 and 1 (probabilities). The model takes the form
\begin{equation}
\label{logit_eq}
P(\text{default}=1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k)}}
\end{equation}
where the coefficients \(\beta\) represent the individual contribution of each variable to the likelihood of default.

On the other hand, a scorecard model was implemented to serve as a more transparent comparison to the logistic regression. After the variables are initially binned and converted into their respective WoE values, each bin of each variable was assigned a fixed number of score points that reflect its contribution to the credit risk. The scorecard is derived from the logistic regression, which models the log-odds of default as:
\begin{equation}
\label{logodds_eq}
\log\left(\frac{P(Y=1 \mid X)}{1 - P(Y=1 \mid X)}\right)
= \beta_{0} + \sum_{j=1}^{k} \beta_{j} \, WoE_{j}
\end{equation}
Each bin-specific score is then calculated as the scaled product of the corresponding regression coefficient and WoE value,
\begin{equation}
\label{score_bin_eq}
\text{Score}_{j,b} = \text{Factor} \cdot \beta_{j} \cdot WoE_{j,b}
\end{equation}
and combined with a base score derived from the regressional intercept. The final score of each individual applicant is therefore calculated as
\begin{equation}
\label{total_score_eq}
\text{Total Score} = \text{BaseScore} + \sum_{j=1}^{k} \text{Score}_{j,b(j)}
\end{equation}
where \(b(j)\) denotes the bin assigned to variable \(X_j\). The full scorecard was implemented using the \(scorecard()\) function from the eponymous package in R.

To evaluate and the performance of these models, a selected number of accuracy metrics, namely Area Under the ROC Curve (AUC) and the Gini coefficient, was used. These metrics measure how well the model is able to differentiate between defaulters and non-defaulters. Furthermore, the Root Mean Squared Error (RSME) quantifies the models' accuracy in their predicted probabilities by quantifying the average deviation between predicted and actual outcomes (in the train set). Additionally, matrices summarizing the performance by reporting true positives, false positives, true negatives and false negatives were generated (confusion matrices). The stability or robustness of the models' scores and probability distributions were assessed with the Population Stability Index (PSI). This metric compares the distributions in the training set with those of the validation set to detect shifts in population characteristics across different samples. In this context, a low score is indicative of a stable model whereas high values suggest a potential drift or deterioration.

\section{Data Selection and Preparation}\label{data-selection-and-preparation}

To train the models to accurately predict the creditworthiness, it is imperative to select predictor variables based on their relevance to the individuals' financial behaviour.

To get a better understanding of the data structure, Table 1 gives an insight into the German dataset.

\begin{table}[!h]
\centering
\caption{\label{tab:dataTable}Exemplary View of the Data Table}
\centering
\begin{tabular}[t]{lcc}
\toprule
creditability & status.of.existing.checking.account & duration.in.month\\
\midrule
good & ... < 0 DM & 6\\
bad & 0 <= ... < 200 DM & 48\\
good & no checking account & 12\\
good & ... < 0 DM & 42\\
bad & ... < 0 DM & 24\\
\addlinespace
good & no checking account & 36\\
\bottomrule
\end{tabular}
\end{table}

The selection process focused on variables that reflect past repayment behaviour, financial stability, and loan characteristics. Restricting the model to these input variables aims to produce transparent and robust results that reflect the real world risk modeling practices. To do so, each variable from the German dataset was evaluated on its Information Value (IV) to determine how well they are suited in discriminating between good and bad borrowers (data quality). Variables with high scores were retained for the subsequent modeling and further investigated in their underlying structuring. In practical terms, a higher IV relates to stronger predictive power of the underlying variable, with general thresholds of being good predictors of values larger than 0.3. Table 2 visualizes the calculated IV scores for the selected variables.

The five selected predictor variables are:

\begin{itemize}

\item \textbf{status.of.existing.checking.account} (categorical) \\
This is a categorical variable that describes the applicant's checking account condition using qualitative labels such as "no checking account", "<0 DM", etc. 

\item \textbf{duration.in.month} (numeric) \\
This is a numeric variable indicating the length of the loan contract in months, reflecting how long the applicant will take to repay off the credit.

\item \textbf{credit.history} (categorical) \\
This is another categorical variable that describes the applicant's past repayment behaviour, ranging from values of "no credits taken" and "all credits in this bank paid back duly" to "critical account".

\item \textbf{savings.account.and.bonds} (categorical) \\
This is another variable that categorises the applicant's savings level, both in their savings account and bonds. There are several categories such as "unknown/no savings account" to " < 100 DM" to separate those into groups.

\item \textbf{purpose} (categorical) \\
Purpose reflects a categorical variable that gives insight into what the applicant intends to do with the credit. Values range from "used car" and "new car" to "education", etc. 
\end{itemize}

\begin{table}[!h]
\centering
\caption{\label{tab:dataIVTable}Information Value of Variables}
\centering
\begin{tabular}[t]{lc}
\toprule
Variable & Information Value\\
\midrule
status.of.existing.checking.account & 0.67\\
duration.in.month & 0.33\\
credit.history & 0.29\\
savings.account.and.bonds & 0.20\\
purpose & 0.17\\
\bottomrule
\end{tabular}
\end{table}

In the following steps, the data was filtered to exclude rows with missing values and split into train and validation set. The split chosen was at a ratio of .75 to .25. This is being done to have two independent samples for training and validating the model at a later point.

\section{Weight-Of-Evidence (WoE) Binning and Transformation}\label{weight-of-evidence-woe-binning-and-transformation}

The binning process was conducted using the \textit{woebin()} function from the \textit{scorecard} package in R and applied separately to both training and validation set as to prevent information leakage, in which certain characteristics of the training set might bias the validation set. The used function supports both categorical and numerical variables with an algorithm selecting the most appropriate method based on the underlying data distribution. As for the selected predictor variables, the variable \textit{duration.in.months}, a continuous variable, was binned using the default ``width'' method, grouping ranges into intervals based on the distribution characteristics. On the other hand, the categorical variable \textit{credit.history} was grouped by merging categories with similar risk profiles until meaningful bins emerged.
After determining the structure, both datasets were transformed by replacing the raw predictor values with the corresponding WoE scores. The resulting datasets therefore consisted exclusively of numerical variables with normalized differences across variables, which serve as a robust basis for the further applied methods of logistic regression and scorecard modelling. Furthermore, the transformation results in the beneficial outcome of transparency, as the WoE values are easily interpretable: higher WoE values indicate lower risk.

\section{Model Implementation}\label{model-implementation}

\subsection{Logistic Regression}\label{logistic-regression}

Following the WoE transformation of the datasets, a logistic regression model was employed to model the log-odds of an invidiual defaulting as a linear function of the selected predictor variables. The WoE transformed values are now used as the predictors \(X_1, X_2,...X_k\) as outlined in equation 2 in the chapter Methodology. As each \(X_i\) is expressed in WoE units, the coefficients \(\beta_i\) directly represent the change in log-odds of default for each unit change in WoE, allowing for a more clear interpretation. The model was employed in R using the \textit{glm()} function.

\subsection{Scorecard Model}\label{scorecard-model}

The scorecard model was constructed following the regression analysis. After the variables from the German datasets were binned and transformed into WoE values, each bin was assigned a fixed score that reflect its contribution to the log-odds of default, with the number of points proportional to the product of the corresponding regression coefficient and its WoE value (Equation 4 in Methodology). All WoE binning plots for the selected variables are included in the Appendix for completeness.

\section{Reporting of Empirical Results}\label{reporting-of-empirical-results}

\subsection{Selected Predictors --- Structure of Probability of Default}\label{selected-predictors-structure-of-probability-of-default}

The selected predictors were determined and visualized earlier using the \textbf{Information Value (IV)}:

\begin{table}[!h]
\centering
\caption{\label{tab:dataIVTable2}Information Value of Variables}
\centering
\begin{tabular}[t]{lc}
\toprule
Variable & Information Value\\
\midrule
status.of.existing.checking.account & 0.67\\
duration.in.month & 0.33\\
credit.history & 0.29\\
savings.account.and.bonds & 0.20\\
purpose & 0.17\\
\bottomrule
\end{tabular}
\end{table}

\emph{Interpretation:}\\
- \texttt{status.of.existing.checking.account} shows the highest IV and is therefore the strongest predictor.\\
- \texttt{duration.in.month} and \texttt{credit.history} show medium predictive power.\\
- \texttt{savings.account.and.bonds} and \texttt{purpose} remain relevant and are retained in the modelling process.

\subsubsection{Structure of PD by Bins (example: duration.in.month)}\label{structure-of-pd-by-bins-example-duration.in.month}

The WoE binning structure already produced earlier can be referenced directly:

\begin{table}[!h]
\centering
\caption{\label{tab:unnamed-chunk-6}Duration-in-Month Bin Structure (Counts, PD, WoE, Points)}
\centering
\begin{tabular}[t]{lccccccclccc}
\toprule
variable & bin & count & count\_distr & neg & pos & posprob & woe & bin\_iv & total\_iv & breaks & is\_special\_values\\
\midrule
duration.in.month & {}[-Inf,8) & 56 & 0.088 & 50 & 6 & 0.107 & -1.247 & 0.099 & 0.312 & 8 & FALSE\\
duration.in.month & {}[8,16) & 214 & 0.337 & 166 & 48 & 0.224 & -0.367 & 0.042 & 0.312 & 16 & FALSE\\
duration.in.month & {}[16,26) & 204 & 0.321 & 145 & 59 & 0.289 & -0.026 & 0.000 & 0.312 & 26 & FALSE\\
duration.in.month & {}[26,44) & 108 & 0.170 & 64 & 44 & 0.407 & 0.499 & 0.046 & 0.312 & 44 & FALSE\\
duration.in.month & {}[44, Inf) & 53 & 0.083 & 23 & 30 & 0.566 & 1.139 & 0.124 & 0.312 & Inf & FALSE\\
\bottomrule
\end{tabular}
\end{table}

\emph{Interpretation:}\\
The PD (positive probability) across bins shows a \textbf{monotonic increase} with longer loan durations.\\
This is ideal behaviour for a scorecard variable and supports its inclusion in the model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Logistic Regression Model --- Parameter Estimates, z-values, Significance}\label{logistic-regression-model-parameter-estimates-z-values-significance}

\subsubsection{WoE-based Logistic Regression (preferred model)}\label{woe-based-logistic-regression-preferred-model}

The regression results have already been produced through:

\begin{table}[!h]
\centering
\caption{\label{tab:unnamed-chunk-7}WoE-based Logistic Regression Coefficients (Estimate, Std. Error, z-value, p-value)}
\centering
\begin{tabular}[t]{llccc}
\toprule
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\midrule
(Intercept) & -0.8628 & 0.1023 & -8.4351 & 0e+00\\
status.of.existing.checking.account\_woe & 0.8195 & 0.1310 & 6.2545 & 0e+00\\
duration.in.month\_woe & 0.9772 & 0.1897 & 5.1521 & 0e+00\\
credit.history\_woe & 0.7643 & 0.1754 & 4.3565 & 0e+00\\
savings.account.and.bonds\_woe & 0.8944 & 0.2697 & 3.3164 & 9e-04\\
\addlinespace
purpose\_woe & 0.9840 & 0.2506 & 3.9274 & 1e-04\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key observations:}
- All coefficients are statistically significant (p-values shown in the table).\\
- Signs and magnitudes align with expectations from the IV and bin structures.\\
- The AIC reported earlier (\texttt{data\_woe.glm\$aic}) confirms model compactness.

\subsubsection{Original-variable Logistic Regression (subset)}\label{original-variable-logistic-regression-subset}

These results were previously printed via:

\begin{table}[!h]
\centering
\caption{\label{tab:unnamed-chunk-8}Original-variable GLM (Selected Predictors)}
\centering
\begin{tabular}[t]{llcc}
\toprule
  & Estimate & Prob\_z & Stars\\
\midrule
(Intercept) & 0.3675 & 0.4879 & \\
status.of.existing.checking.account0 ... & -0.3628 & 0.1143 & \\
status.of.existing.checking.account..... & -1.2535 & 0.0061 & **\\
status.of.existing.checking.accountno... & -1.8571 & 0.0000 & ***\\
duration.in.month & 0.0335 & 0.0000 & ***\\
\addlinespace
credit.historyall credits at this ban... & -0.5436 & 0.3754 & \\
credit.historyexisting credits paid b... & -1.1827 & 0.0157 & *\\
credit.historydelay in paying off in ... & -1.0068 & 0.0709 & .\\
credit.historycritical account/ other... & -1.9179 & 0.0002 & ***\\
\bottomrule
\end{tabular}
\end{table}

The original-variable GLM confirms:
- Significant effects from checking-account categories\\
- Duration remains highly significant\\
- Demonstrates consistency with the WoE model

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{c) Scorecard Validation --- Gini IS \& OoS}\label{c-scorecard-validation-gini-is-oos}

Metrics have already been generated by:

\begin{table}[!h]
\centering\caption{\label{tab:unnamed-chunk-9}Probability-Prediction Metrics (RMSE, AUC, Gini)}
\begin{table}

\centering
\begin{tabular}[t]{lcc}
\toprule
RMSE & AUC & Gini\\
\midrule
0.3988 & 0.8022 & 0.6045\\
\bottomrule
\end{tabular}
\end{table}\begin{table}

\centering
\begin{tabular}[t]{lcc}
\toprule
RMSE & AUC & Gini\\
\midrule
0.4153 & 0.7774 & 0.5548\\
\bottomrule
\end{tabular}
\end{table}
\end{table}

And for scorecard points:

\begin{table}[!h]
\centering\caption{\label{tab:unnamed-chunk-10}Score-Prediction Metrics (RMSE, AUC, Gini)}
\begin{table}

\centering
\begin{tabular}[t]{lc}
\toprule
AUC & Gini\\
\midrule
0.802 & 0.6039\\
\bottomrule
\end{tabular}
\end{table}\begin{table}

\centering
\begin{tabular}[t]{lc}
\toprule
AUC & Gini\\
\midrule
0.7779 & 0.5558\\
\bottomrule
\end{tabular}
\end{table}
\end{table}

\textbf{Interpretation:}
- \textbf{In-sample Gini \textasciitilde{} 0.60}\\
- \textbf{Out-of-sample Gini \textasciitilde{} 0.55}\\
- The small drop between IS and OoS indicates stable generalization.

\subsubsection{Population Stability Index (PSI)}\label{population-stability-index-psi}

\begin{table}[!h]
\centering
\caption{\label{tab:unnamed-chunk-11}Population Stability Index (PSI)}
\centering
\begin{tabular}[t]{lcl}
\toprule
variable & dataset & psi\\
\midrule
score & train\_validate & 0.0424\\
\bottomrule
\end{tabular}
\end{table}

A PSI around 0.04 indicates \textbf{very stable} population behaviour between train and validation samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Summary \& Practical Benefits}\label{summary-practical-benefits}

\textbf{Summary of Findings}
- IV analysis confirms five strong and relevant predictors.\\
- WoE transformation produces monotonic PD patterns and stable coefficients.\\
- Logistic regression coefficients (all significant) validate predictor usefulness.\\
- Scorecard performance is strong:\\
- IS Gini about 0.60\\
- OoS Gini about 0.55\\
- PSI about 0.04 demonstrates population stability.

\textbf{Who Benefits}
- \textbf{Risk management:} receives model stability evidence (Gini, PSI, IV).\\
- \textbf{Underwriting:} gains an interpretable scorecard with robust discriminatory power.\\
- \textbf{Credit policy:} can adjust rules using transparent variable effects and score contributions.\\
- \textbf{Regulatory/validation teams:} benefit from WoE-based transparency and monotonicity.

\section{Appendix}\label{appendix}

\newpage

\bibliography{literature.bib}

\end{document}
