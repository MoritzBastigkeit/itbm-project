---
title: >
  \begin{center}
  Predictive Analytics: Application in the Credit Risk Domain \\
  Case Study Teaching (CST)-Vignette in cheat sheet style \\
  ("group project cover sheet")
  \end{center}
author: "Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha"
date: "Nov. 27, 2025 (Scorecard_Intro_2511.Rmd)"
site: bookdown::bookdown_site
output:
  bookdown::pdf_document2:
 #pdf_document:
    toc: yes
    number_sections: true
    citation_package: natbib
    latex_engine: pdflatex
    keep_tex: true
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\Large}
  - \posttitle{\par\end{center}}
bibliography: literature.bib
biblio-style: apalike
include-before:
  - \pagebreak
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\newpage

# Abstract

Credit risk assessment is a central task in retail banking, ensuring that
financial institutions can effectively discriminate between creditworthy
and non-creditworthy applicants. Modern credit scoring increasingly
relies on data-driven methods that combine statistical modelling,
machine learning techniques and domain-specific transformations to
produce interpretable and stable credit-risk predictions.  
This project applies a **Weight-of-Evidence (WoE)** and
**Information Value (IV)**–driven scorecard modelling framework using
the *German Credit* dataset. The workflow includes data preparation,
IV-based feature assessment, supervised binning, WoE transformation,
logistic regression modelling, scorecard generation and out-of-sample
validation. Performance is evaluated through stability measures such as
the **Population Stability Index (PSI)** and accuracy metrics including
AUC, Gini and RMSE. The resulting scorecard provides a transparent,
regulator-compliant and empirically robust tool for credit-risk
prediction.

# Introduction

Credit risk modelling aims to quantify the likelihood that a borrower
will fail to meet contractual repayment obligations. As banks, insurers
and financial intermediaries increasingly rely on data-driven decision
frameworks, scorecards have become the industry standard for credit
underwriting due to their transparency, interpretability and regulatory
acceptance \citep{HandHenley1997, Thomas2002}. In contrast to purely
algorithmic black-box models, scorecards preserve a clear link between
economic reasoning, data transformations and model parameters—an aspect
that is essential for auditability and explainability in regulated
environments.


comment: my introduction, might merge it with this 
A central function of banks is to provide loans to individuals and companies. Credit scoring models are an essential tool for banks to assess the creditworthiness of lenders and predict how likely they are to meet their financial obligations. Popular models are based on the 3C's, 4C's, or 5C's, which stand for character, capital, collateral, capacity, and condition. However, with advancing technology, more novel approaches have emerged. In the subsequent paper, a credit scoring model is built and evaluated based on German data. The objective of the paper is to investigate the predictive value of demographic attributes. The objective of this study is not to enhance accuracy, but rather to offer insight into the structure of creditworthiness within the German context.

The aim of this project is to build and validate a **predictive
scorecard model** based on the *German Credit* dataset.
To achieve this, the project adopts a structured analytical workflow
aligned with established credit-risk modelling guidelines
\citep{Anderson2007, Siddiqi2017}. The key methodological components are:

1. **Data Filtering and Variable Selection** – Choosing a
   subset of variables that capture behavioural and financial
   characteristics of borrowers.
2. **Information Value Analysis (IV)** – Quantifying the predictive strength
   of candidate variables.
3. **Supervised Binning and Weight-of-Evidence Transformation (WoE)** –
   Creating monotonic predictor–default relationships and ensuring stable
   logistic-regression estimation.
4. **Logistic Regression Modelling** – Fitting a parsimonious and
   interpretable model linking WoE-transformed predictors to the
   probability of default.
5. **Scorecard Generation** – Translating the regression coefficients and
   bin structures into a practical scorecard with additive scorepoints.
6. **Model Validation** – Assessing predictive accuracy (AUC, Gini, RMSE),
   calibration, and population stability (PSI) for both in-sample (IS)
   and out-of-sample (OoS) samples.

Using WoE transformations is beneficial because it enforces
monotonicity, reduces the influence of outliers and yields logistic
models with minimal multicollinearity \citep{Siddiqi2017}. This
ensures that the final scorecard is both empirically robust.

Overall, this project demonstrates how predictive analytics can be
applied to credit-risk modelling to produce a validated scorecard.
The methodological steps should lead to a replicable blueprint for building
credit-risk models.

# Methodology
In order to build and assess these models, the Weight-of-Evidence (WoE) approach was taken. In it, the raw features of individuals are transformed in that they are sorted in bins based on their predictive strength to distinguish between defaulters and non-defaulters. To calculate it, the logarithm of the ratio of non-defaulters to defaulters within a group is taken, resulting in a score that represents credit risk. In doing so, the WoE approach allows to convert categorical or numerical values into values that are then processed in a logistic regression or scorecard model. In the underlying general formula, the event is defined in the underlying case as being a \textit{good customer}, meaning that this individual repays their debt towards the bank, respectively in each group $i$.
\begin{equation}
\label{woe_eq}
WoE_i = \ln\left(\frac{Event_i\%}{NonEvent_i\%}\right)
\end{equation}
The resulting predictor variables were then used in two models, first a logistic regression and secondly a scorecard model, to compare the accuracy of each. Both models aim to predict the probability of default $P(Y=1)$ for the given set of predictors with the logistic approach using a sigmoid function. Sigmoid functions are mathematical functions, usually shaped like an S, used for classification problems by taking real numbers as an input and transforming them to values between 0 and 1 (probabilities). The model takes the form$$P(default=1|X) = \frac{1}{1+e^{-(\beta_0+\beta_1X_1+...+\beta_kX_k)}}$$ where the coefficients $\beta$ represent the individual contribution of each variable to the likelihood of default.

On the other hand, a scorecard model was implemented to serve as a more transparent comparison to the logistic regression. After the variables are initially binned and converted into their respective WoE values, each bin of each variable was assigned a fixed number of score points that reflect its contribution to the credit risk. This was done by using the $scorecard()$ function from the eponymous package in R. For any individual applicant, the total credit score was then calculated by summing up the score points across their variables with higher scores representing lower risk of defaulting. 

To evaluate and the performance of these models, a selected number of accuracy metrics, namely Area Under the ROC Curve (AUC) and the Gini coefficient, was used. These metrics measure how well the model is able to differentiate between defaulters and non-defaulters. Furthermore, the Root Mean Squared Error (RSME) quantifies the models' accuracy in their predicted probabilities by quantifying the average deviation between predicted and actual outcomes (in the train set). Additionally, matrices summarizing the performance by reporting true positives, false positives, true negatives and false negatives were generated (confusion matrices). The stability or robustness of the models' scores and probability distributions were assessed with the Population Stability Index (PSI). This metric compares the distributions in the training set with those of the validation set to detect shifts in population characteristics across different samples. In this context, a low score is indicative of a stable model whereas high values suggest a potential drift or deterioration.

```{r, echo = FALSE}
library(scorecard)
library(tidyverse)
library(knitr)
library(kableExtra)

data("germancredit") 

data.df <- germancredit %>% select(
creditability,
status.of.existing.checking.account,
duration.in.month,
credit.history,
savings.account.and.bonds,
purpose
)

iv.df <- iv(data.df, y = "creditability")
```


# Data Selection

To train the models to accurately predict the creditworthiness, it is imperative to select predictor variables based on their relevance to the individuals' financial behaviour. 

To get a better understanding of the data structure, Table 1 gives an insight into the German dataset.
```{r dataTable, echo=FALSE}
data.df[,1:3] %>% 
  head() %>% 
  kbl(
    caption = "Exemplary View of the Data Table",
    digits = 2,
    align = c("l", "c", "c", "c"),
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = "hold_position",
    full_width = FALSE
  )
```

The selection process focused on variables that reflect past repayment behaviour, financial stability, and loan characteristics. Restricting the model to these input variables aims to produce transparent and robust results that reflect the real world risk modeling practices. To do so, each variable from the German dataset was evaluated on its Information Value (IV) to determine how well they are suited in discriminating between good and bad borrowers (data quality). Variables with high scores were retained for the subsequent modeling and further investigated in their underlying structuring. In practical terms, a higher IV relates to stronger predictive power of the underlying variable, with general thresholds of being good predictors of values larger than 0.3. Table 2 visualizes the calculated IV scores for the selected variables.


The five selected predictor variables are:
\begin{itemize}

\item \textbf{status.of.existing.checking.account} (categorical) \\
This is a categorical variable that describes the applicant's checking account condition using qualitative labels such as "no checking account", "<0 DM", etc. 

\item \textbf{duration.in.month} (numeric) \\
This is a numeric variable indicating the length of the loan contract in months, reflecting how long the applicant will take to repay off the credit.

\item \textbf{credit.history} (categorical) \\
This is another categorical variable that describes the applicant's past repayment behaviour, ranging from values of "no credits taken" and "all credits in this bank paid back duly" to "critical account".

\item \textbf{savings.account.and.bonds} (categorical) \\
This is another variable that categorises the applicant's savings level, both in their savings account and bonds. There are several categories such as "unknown/no savings account" to " < 100 DM" to separate those into groups.

\item \textbf{purpose} (categorical) \\
Purpose reflects a categorical variable that gives insight into what the applicant intends to do with the credit. Values range from "used car" and "new car" to "education", etc. 
\end{itemize}

```{r dataIVTable, echo = FALSE}
iv.table <- data.df %>%
  iv(y = "creditability")

iv.table %>% 
  kbl(
    caption = "Information Value of Variables",
    digits = 2,
    col.names = c("Variable", "Information Value"),
    align = c("l", "c"),
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("hold_position", "striped"),
    full_width = FALSE
  )

data_f.df <- data.df %>% 
  var_filter("creditability")

data_f.list <- data_f.df %>% 
  split_df("creditability",
           ratios   = c(0.65, 0.35),
           name_dfs = c("train", "validate"))
default.list <- data_f.list %>% 
  lapply(function(x) x$creditability)

```


In the following steps, the data was filtered to exclude rows with missing values and split into train and validation set. The split chosen was at a ratio of .75 to .25. This is being done to have two independent samples for training and validating the model at a later point.


# Weight-Of-Evidence (WoE)-based transformation of predictor variables

## WoE-based binning of train and validate samples: bins.list

WoE-based classing, i.e. binning and grouping of predictor variables

```{r}
bins.list <- data_f.list$train %>% 
  woebin("creditability") 
```

**Hint**: The default binning method is method="width". Other methods are

-   "frequ" that support numerical variables as well as

-   "tree" and "chimerge" supporting both, i.e. numerical and
    categorical variables which are used in the optimal binning
    approach.

```{r}
bins.list %>% names()
```

Plotting the bins (including bin statistics)

```{r fig.height=2.5, fig.align='center'}
bins.list$status.of.existing.checking.account %>% 
  woebin_plot()
```

**Hint**: credit.amount does not have an acceptable structure of the default rates (positive probability) over the bins like e.g. a linear or u-curve structure; hence it should not be included in the scorecard model!

```{r fig.height=2.5, fig.align='center'}
bins.list$duration.in.month %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$credit.history %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$savings.account.and.bonds %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$purpose %>% 
  woebin_plot()
```

**Hint**: duration.in.month has lineare structure of the default rates; hence it should be included in the scorecard model!

**Excursion**: Manual bin-adjustments

Bins can be altered manually by

1.  Saving the bin list generated in the woebin() function via e.g.
    save_as="breaks2410.list"

2.  Loading the saved R-file "breaks2410.list.R", editing the breaks as
    needed and storing the file

3.  Sourcing the edited and stored "breaks2410.list.R" file with the
    "source(...)\$value" function

4.  Binning the data again with the "woebin()" function with the
    additional argument "break_list"

ad 1)

```{r eval=FALSE}
bins.list <- data_f.list$train %>% 
  woebin("creditability",
         save_as = "breaks2410.list") 
```

ad 3)

```{r eval=FALSE}
breaksList <- source("breaks2410.list.R")$value
```

ad 4)

```{r eval=FALSE}
bins.list <- data_f.list$train %>% 
  woebin("creditability",
         breaks_list = "breaksList") 
```

**Hint**: The above code chunks are not yet evaluated, as they are
performed only when the original binning does not deliver beneficial
results.

## WoE-based transforming of predictor variables: data_woe.list

### WoE-based transforming of train and validate data: data_woe.list

Transforming splitted sample: Needed for train/validate analysis

```{r}
data_woe.list <- data_f.list %>% 
  lapply(function(x) woebin_ply(x, bins.list))
```

```{r}
data_woe.list %>% lapply(class)
data_woe.list %>% lapply(dim)
```

```{r}
data_woe.list <- data_f.list %>% 
  lapply(function(x) woebin_ply(x, bins.list))

```

\newpage

# Generalized linear model (glm): Regressing predictors against responses

## Logistic regression of WoE-transformed predictors: glm(.,data_woe.list\$train)

The WoE-based logistic regression is the preferred regression approach
as it delivers the most compact regression models.

### Constructing and calibrating the WoE-based logistic regression model

```{r}
data_woe.glm <- glm(
  creditability ~ .,
  family = binomial(),
  data = data_woe.list$train
)
```

### Investigating the fitted regression model

```{r}
data_woe.glm$aic
```

Summary of regression: summary()

```{r}
summary(data_woe.glm)
```

## Logistic regression of original predictors: glm(.,data_f.list\$train)

```{r}

data_f.glm <- glm(
  creditability ~ status.of.existing.checking.account + duration.in.month + credit.history,
  family = binomial(),
  data = data_f.list$train
)
```

**Hint**: For simplicity only three original predictors are included in the logistic regression model  

```{r}
data_f.glm$aic
```

```{r}
data_f.glm$xlevels
```

For getting a compact summary the function summary() is customized and formatted 

```{r}
formatSummary <- function(model_summary) {
  aux_coeff <- model_summary$coefficients[, 1]
  aux_prob <- model_summary$coefficients[, 4]
  aux_stars <- symnum(aux_prob, 
                       corr = FALSE, 
                       na = FALSE,
                       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                       symbols = c("***", "**", "*", ".", " "))
  names(aux_coeff) <- str_trunc(names(aux_coeff), 
                                width = 40)
  aux_result <- data.frame(Estimate = aux_coeff, 
                           Prob_z = aux_prob, 
                           "Stars" = aux_stars)
  return(aux_result)
}
```

```{r}
summary(data_f.glm) %>% formatSummary()
```

\newpage

# Building scorecard-models (scm) and calculating scorepoints

Scorepoints are calculated by combing scorecard-model, which combines
bin and glm information, with individual data

## Building scm-models: Combining bins.list & data_woe.glm in scorecard()

Building the scorecard via bin and glm information resulting from train
sample

```{r}
scorecard.scm <- bins.list %>% scorecard(data_woe.glm)
```

```{r}
scorecard.scm %>% names()
```

Investigating the content of the "woe-based" scorecard model

```{r}
scorecard.scm$basepoints
```

```{r}
scorecard.scm$duration.in.month[,1:8]
```

```{r}
scorecard.scm$duration.in.month[,c(1,9:13)]
```

## Calculating scorepoints: Combinig individual data.df & scm-model in scorecard_ply()

Generating a score list

```{r}
score.list <- data_f.list %>% 
  lapply(function(x) scorecard_ply(x, scorecard.scm)) 
```

**Hint**: The only_total_score=TRUE (= default argument) has to be used
for providing two compatible lists for further processing. If scores to
the different predictors are of interest, the two separate, i.e. train
and validate samples have to analyzed individually with the argument
only_total_score=FALSE.

```{r}
score.list %>% names()
```

```{r}
score.list$train %>%
  head()
```

```{r}
score.list$validate %>%
  head()
```

\newpage

# WoE-based predicting (forecasting) of probabilities and scorepoints

## Predicting probabilities: Combining data_woe.list & data_woe.glm in predict()

```{r}
predProb.list <- data_woe.list %>% 
  lapply(function(x) predict(data_woe.glm,
                             type = 'response',
                             x))
```

**Hint**: Due to the fact that the data_woe.glm was calibrated for the
train sample two different types of prediction can be destinguished,
i.e. the in-sample (IS) prediction by using the train sample in the
predict()-function, and the out-of-sample (OoS) prediction by using the
test sample in the predict()-function.

```{r}
predProb.list %>% names()
```

```{r}
predProb.list$train %>% head() # In-Sample prediction
```

```{r}
predProb.list$validate %>% head() # Out-of-Sample prediction
```

## Predicting scorepoints: Retrieving predictions from score.list generated in scorecard_ply()

The prediction of the scorepoints is alread incorported in the built
scorecard.

```{r}
score.list$train %>% 
  head()
```

```{r}
score.list$validate %>% 
  head()
```

\newpage

# Scorecard Validation: Statistical testing of forecasting accuracy

## Checking stability of score and probility distributions: Population Stability Index (PSI)

```{r fig.height=2.5}
psi.list <- perf_psi(score = score.list, 
                     label = default.list,
                     return_distr_dat = TRUE)
```

**Hint**: More details of per_psi() function are given \@
<https://www.rdocumentation.org/packages/scorecard/versions/0.1.9/topics/perf_psi>

```{r fig.height=3}
psi.list$pic
```

```{r}
psi.list %>% names()
psi.list$dat %>% names()
psi.list$dat$score[,1:9] %>% head()
psi.list$psi
```

```{r eval=FALSE}
perf_psi(score, label = NULL, title = NULL, x_limits = NULL,
  x_tick_break = 50, show_plot = TRUE, seed = 186,
  return_distr_dat = FALSE)
# e.g. # x_limits = c(250, 700),
#      # x_tick_break = 50,
```

## IS & OoS testing probability prediction accuracy: perf_eva(.,predProb.list)

probability prediction accuracy

```{r}
ProbPredAccuracy <- perf_eva(pred = predProb.list,
                             label = default.list,
                             binomial_metric = c("rmse","auc","gini"),
                             show_plot=c("roc","ks"),
                             confusion_matrix = TRUE)
```

```{r}
names(ProbPredAccuracy)
```

```{r}
ProbPredAccuracy$binomial_metric
```

```{r}
ProbPredAccuracy$confusion_matrix
```

Excursion

```{r}
perf_eva(pred = predProb.list, 
         label = default.list,
         binomial_metric = c("rmse","auc","gini"),
         show_plot= FALSE,
         confusion_matrix = FALSE)
```

## IS & OoS testing scorepoint prediction accuracy: perf_eva(.,score.list)

scorepoint prediction accuracy

```{r}
ScorePredAccuracy <- perf_eva(pred = score.list,
                             label = default.list,
                             binomial_metric = c("rmse","auc","gini"),
                             show_plot=c("roc","ks"),
                             confusion_matrix = TRUE)
```

```{r}
names(ScorePredAccuracy)
```

```{r}
ScorePredAccuracy$binomial_metric
```

```{r}
ScorePredAccuracy$confusion_matrix
```

\newpage

# Appendix

## Appendix: Essay style with formulas in LaTeX language

**Group project assignment**: Write a scholarly essay with full
sentences, correct citations and LaTeX formulas.

**Example essay style**: From a statistical perspective the transition
from the $MPS$ to the VaR framework is related to switching the
perspective from considering moments (parameters) of random variables,
i.e. $\mu$ and $\sigma$, to considering the quantiles and corresponding
probabilities of these variables. Specifically, the VaR measure
specifies the risk of a random variable ($\tilde{P}$) via the threshold
quantile ($VaR$) that is exceeded into the negative direction (i.e.
$P \leq VaR$) with the loss probability ($\alpha$) or respectively, is
exceeded into the positive direction (i.e. $P > VaR$) with the
complementary probability, i.e. the confidence level ($1-\alpha$).

## Appendix: Generating tables, figures, cross references and citations

```{r testFigure, fig.cap="Amount vs. Duration", fig.width=4, fig.height=3, fig.align='center'}
data.df[1:100,2:3] %>% plot()
```

Formulas without numbering \begin{align*}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align*}

Formulas with numbering (and labeling which is needed for referencing)
\begin{align} \label{eq:VaR}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align}

Formula \@ref(eq:VaR) is a sample formula defining the Value at Risk.

Always cite original literature to avoid plagiarism: e.g.
\cite{SchwaigerIUF} or \citep{SchwaigerIUF}. Don't forget to cite page
numbers as well for literal citations, e.g. \cite[p. 100]{SchwaigerIUF}.

\newpage
