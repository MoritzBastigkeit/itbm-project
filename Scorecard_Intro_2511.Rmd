---
title: >
  \begin{center}
  Predictive Analytics: Application in the Credit Risk Domain \\
  Case Study Teaching (CST)-Vignette in cheat sheet style \\
  ("group project cover sheet")
  \end{center}
author: "Bastigkeit Moritz, Ennser Valentin, Grabherr Elias, Nafees Muhammad Talha"
date: "Nov. 27, 2025 (Scorecard_Intro_2511.Rmd)"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    number_sections: true
    citation_package: natbib
bibliography: literature.bib
biblio-style: apalike
include-before:
  - \pagebreak
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\newpage

# Abstract

Credit risk assessment is a central task in retail banking, ensuring that
financial institutions can effectively discriminate between creditworthy
and non-creditworthy applicants. Modern credit scoring increasingly
relies on data-driven methods that combine statistical modelling,
machine learning techniques and domain-specific transformations to
produce interpretable and stable credit-risk predictions.  
This project applies a **Weight-of-Evidence (WoE)** and
**Information Value (IV)**–driven scorecard modelling framework using
the *German Credit* dataset. The workflow includes data preparation,
IV-based feature assessment, supervised binning, WoE transformation,
logistic regression modelling, scorecard generation and out-of-sample
validation. Performance is evaluated through stability measures such as
the **Population Stability Index (PSI)** and accuracy metrics including
AUC, Gini and RMSE. The resulting scorecard provides a transparent,
regulator-compliant and empirically robust tool for credit-risk
prediction.

# Introduction

Credit risk modelling aims to quantify the likelihood that a borrower
will fail to meet contractual repayment obligations. As banks, insurers
and financial intermediaries increasingly rely on data-driven decision
frameworks, scorecards have become the industry standard for credit
underwriting due to their transparency, interpretability and regulatory
acceptance \citep{HandHenley1997, Thomas2002}. In contrast to purely
algorithmic black-box models, scorecards preserve a clear link between
economic reasoning, data transformations and model parameters—an aspect
that is essential for auditability and explainability in regulated
environments.

The aim of this project is to build and validate a **predictive
scorecard model** based on the *German Credit* dataset.
To achieve this, the project adopts a structured analytical workflow
aligned with established credit-risk modelling guidelines
\citep{Anderson2007, Siddiqi2017}. The key methodological components are:

1. **Data Filtering and Variable Selection** – Choosing a
   subset of variables that capture behavioural and financial
   characteristics of borrowers.
2. **Information Value Analysis (IV)** – Quantifying the predictive strength
   of candidate variables.
3. **Supervised Binning and Weight-of-Evidence Transformation (WoE)** –
   Creating monotonic predictor–default relationships and ensuring stable
   logistic-regression estimation.
4. **Logistic Regression Modelling** – Fitting a parsimonious and
   interpretable model linking WoE-transformed predictors to the
   probability of default.
5. **Scorecard Generation** – Translating the regression coefficients and
   bin structures into a practical scorecard with additive scorepoints.
6. **Model Validation** – Assessing predictive accuracy (AUC, Gini, RMSE),
   calibration, and population stability (PSI) for both in-sample (IS)
   and out-of-sample (OoS) samples.

Using WoE transformations is beneficial because it enforces
monotonicity, reduces the influence of outliers and yields logistic
models with minimal multicollinearity \citep{Siddiqi2017}. This
ensures that the final scorecard is both empirically robust.

Overall, this project demonstrates how predictive analytics can be
applied to credit-risk modelling to produce a validated scorecard.
The methodological steps should lead to a replicable blueprint for building
credit-risk models.

# Data Loading and Preparation

## Loading libraries

```{r}
library(scorecard)
library(tidyverse)
library(knitr)
```

## Loading external data

```{r}
data("germancredit") 
```

## Data Selection

To train our models to predict the creditworthiness, the following variables were selected as they capture key aspects of an applicant's financial situation.

The five predictor variables are:

**status.of.existing.checking.account)** (categorical)
This is a categorical variable that describes the applicant's checking account condition using qualitative labels such as "no checking account", "<0 DM", etc. 

**duration.in.month)** (numeric)
This is a numeric variable indicating the length of the loan contract in months, reflecting how long the applicant will take to repay off the credit.

**credit.history** (categorical)
This is another categorical variable that describes the applicant's past repayment behaviour, ranging from values of "no credits taken" and "all credits in this bank paid back duly" to "critical account".

**savings.account.and.bonds** (categorical)
This is another variable that categorises the applicant's savings level, both in their savings account and bonds. There are several categories such as "unknown/no savings account" to " < 100 DM" to separate those into groups.

**purpose** (categorical)
Purpose reflects a categorical variable that gives insight into what the applicant intends to do with the credit. Values range from "used car" and "new car" to "education", etc. 


```{r, include = FALSE}
data.df <- germancredit %>% select(
creditability,
status.of.existing.checking.account,
duration.in.month,
credit.history,
savings.account.and.bonds,
purpose
)

iv.df <- iv(data.df, y = "creditability")
```

## Information Values
In a first step, the information value of the predictor variables is regarded to get a general overview of the data quality. This information value (IV) measures how well each predictor variable can separate "good borrowers" from "bad borrowers" (in relation to creditability). In practical terms, a higher IV relates to stronger predictive power of the underlying variable, with general thresholds of being good predictors of values larger than 0.3. 

```{r, echo = FALSE}
library(kableExtra)

data.df %>%
iv(y = "creditability") %>%
kable(
align = 'lccc',
digits = 2,
caption = "Information Value of Predictor Variables"
) %>%
kable_styling(latex_options = "HOLD_position")
```

## Filtering and Splitting Data

In the following step, the data is filtered to exclude rows with missing values and split into train and validation set. The split chosen is at a ratio of .65 to .35. This is being done to have two independent samples for training and validating the model at a later point.

```{r, echo = FALSE}
data_f.df <- data.df %>% 
  var_filter("creditability")

data_f.list <- data_f.df %>% 
  split_df("creditability",
           ratios   = c(0.65, 0.35),
           name_dfs = c("train", "validate"))
```

## Specifying dummy variable for credit defaults: default.list

For being able to statistically analyze and test the results from the
scorecard the default.list is established that contains the default
values of the response variable

```{r}
default.list <- data_f.list %>% 
  lapply(function(x) x$creditability)
```

**Hint**: The default.list is needed for calculating the population
stability indes (PSI) with the function perf_psi() and the gains table
with the function gains_table().

Exemplarily showing the content of the default.list

```{r}
default.list %>% str()
```

\newpage

# Weight-Of-Evidence (WoE)-based transformation of predictor variables

## WoE-based binning of train and validate samples: bins.list

WoE-based classing, i.e. binning and grouping of predictor variables

```{r}
bins.list <- data_f.list$train %>% 
  woebin("creditability") 
```

**Hint**: The default binning method is method="width". Other methods are

-   "frequ" that support numerical variables as well as

-   "tree" and "chimerge" supporting both, i.e. numerical and
    categorical variables which are used in the optimal binning
    approach.

```{r}
bins.list %>% names()
```

Plotting the bins (including bin statistics)

```{r fig.height=2.5, fig.align='center'}
bins.list$status.of.existing.checking.account %>% 
  woebin_plot()
```

**Hint**: credit.amount does not have an acceptable structure of the default rates (positive probability) over the bins like e.g. a linear or u-curve structure; hence it should not be included in the scorecard model!

```{r fig.height=2.5, fig.align='center'}
bins.list$duration.in.month %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$credit.history %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$savings.account.and.bonds %>% 
  woebin_plot()
```

```{r fig.height=2.5, fig.align='center'}
bins.list$purpose %>% 
  woebin_plot()
```

**Hint**: duration.in.month has lineare structure of the default rates; hence it should be included in the scorecard model!

**Excursion**: Manual bin-adjustments

Bins can be altered manually by

1.  Saving the bin list generated in the woebin() function via e.g.
    save_as="breaks2410.list"

2.  Loading the saved R-file "breaks2410.list.R", editing the breaks as
    needed and storing the file

3.  Sourcing the edited and stored "breaks2410.list.R" file with the
    "source(...)\$value" function

4.  Binning the data again with the "woebin()" function with the
    additional argument "break_list"

ad 1)

```{r eval=FALSE}
bins.list <- data_f.list$train %>% 
  woebin("creditability",
         save_as = "breaks2410.list") 
```

ad 3)

```{r eval=FALSE}
breaksList <- source("breaks2410.list.R")$value
```

ad 4)

```{r eval=FALSE}
bins.list <- data_f.list$train %>% 
  woebin("creditability",
         breaks_list = "breaksList") 
```

**Hint**: The above code chunks are not yet evaluated, as they are
performed only when the original binning does not deliver beneficial
results.

## WoE-based transforming of predictor variables: data_woe.list

### WoE-based transforming of train and validate data: data_woe.list

Transforming splitted sample: Needed for train/validate analysis

```{r}
data_woe.list <- data_f.list %>% 
  lapply(function(x) woebin_ply(x, bins.list))
```

```{r}
data_woe.list %>% lapply(class)
data_woe.list %>% lapply(dim)
```

```{r}
data_woe.list <- data_f.list %>% 
  lapply(function(x) woebin_ply(x, bins.list))

```

\newpage

# Generalized linear model (glm): Regressing predictors against responses

## Logistic regression of WoE-transformed predictors: glm(.,data_woe.list\$train)

The WoE-based logistic regression is the preferred regression approach
as it delivers the most compact regression models.

### Constructing and calibrating the WoE-based logistic regression model

```{r}
data_woe.glm <- glm(
  creditability ~ .,
  family = binomial(),
  data = data_woe.list$train
)
```

### Investigating the fitted regression model

```{r}
data_woe.glm$aic
```

Summary of regression: summary()

```{r}
summary(data_woe.glm)
```

## Logistic regression of original predictors: glm(.,data_f.list\$train)

```{r}

data_f.glm <- glm(
  creditability ~ status.of.existing.checking.account + duration.in.month + credit.history,
  family = binomial(),
  data = data_f.list$train
)
```

**Hint**: For simplicity only three original predictors are included in the logistic regression model  

```{r}
data_f.glm$aic
```

```{r}
data_f.glm$xlevels
```

For getting a compact summary the function summary() is customized and formatted 

```{r}
formatSummary <- function(model_summary) {
  aux_coeff <- model_summary$coefficients[, 1]
  aux_prob <- model_summary$coefficients[, 4]
  aux_stars <- symnum(aux_prob, 
                       corr = FALSE, 
                       na = FALSE,
                       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                       symbols = c("***", "**", "*", ".", " "))
  names(aux_coeff) <- str_trunc(names(aux_coeff), 
                                width = 40)
  aux_result <- data.frame(Estimate = aux_coeff, 
                           Prob_z = aux_prob, 
                           "Stars" = aux_stars)
  return(aux_result)
}
```

```{r}
summary(data_f.glm) %>% formatSummary()
```

\newpage

# Building scorecard-models (scm) and calculating scorepoints

Scorepoints are calculated by combing scorecard-model, which combines
bin and glm information, with individual data

## Building scm-models: Combining bins.list & data_woe.glm in scorecard()

Building the scorecard via bin and glm information resulting from train
sample

```{r}
scorecard.scm <- bins.list %>% scorecard(data_woe.glm)
```

```{r}
scorecard.scm %>% names()
```

Investigating the content of the "woe-based" scorecard model

```{r}
scorecard.scm$basepoints
```

```{r}
scorecard.scm$duration.in.month[,1:8]
```

```{r}
scorecard.scm$duration.in.month[,c(1,9:13)]
```

## Calculating scorepoints: Combinig individual data.df & scm-model in scorecard_ply()

Generating a score list

```{r}
score.list <- data_f.list %>% 
  lapply(function(x) scorecard_ply(x, scorecard.scm)) 
```

**Hint**: The only_total_score=TRUE (= default argument) has to be used
for providing two compatible lists for further processing. If scores to
the different predictors are of interest, the two separate, i.e. train
and validate samples have to analyzed individually with the argument
only_total_score=FALSE.

```{r}
score.list %>% names()
```

```{r}
score.list$train %>%
  head()
```

```{r}
score.list$validate %>%
  head()
```

\newpage

# WoE-based predicting (forecasting) of probabilities and scorepoints

## Predicting probabilities: Combining data_woe.list & data_woe.glm in predict()

```{r}
predProb.list <- data_woe.list %>% 
  lapply(function(x) predict(data_woe.glm,
                             type = 'response',
                             x))
```

**Hint**: Due to the fact that the data_woe.glm was calibrated for the
train sample two different types of prediction can be destinguished,
i.e. the in-sample (IS) prediction by using the train sample in the
predict()-function, and the out-of-sample (OoS) prediction by using the
test sample in the predict()-function.

```{r}
predProb.list %>% names()
```

```{r}
predProb.list$train %>% head() # In-Sample prediction
```

```{r}
predProb.list$validate %>% head() # Out-of-Sample prediction
```

## Predicting scorepoints: Retrieving predictions from score.list generated in scorecard_ply()

The prediction of the scorepoints is alread incorported in the built
scorecard.

```{r}
score.list$train %>% 
  head()
```

```{r}
score.list$validate %>% 
  head()
```

\newpage

# Scorecard Validation: Statistical testing of forecasting accuracy

## Checking stability of score and probility distributions: Population Stability Index (PSI)

```{r fig.height=2.5}
psi.list <- perf_psi(score = score.list, 
                     label = default.list,
                     return_distr_dat = TRUE)
```

**Hint**: More details of per_psi() function are given \@
<https://www.rdocumentation.org/packages/scorecard/versions/0.1.9/topics/perf_psi>

```{r fig.height=3}
psi.list$pic
```

```{r}
psi.list %>% names()
psi.list$dat %>% names()
psi.list$dat$score[,1:9] %>% head()
psi.list$psi
```

```{r eval=FALSE}
perf_psi(score, label = NULL, title = NULL, x_limits = NULL,
  x_tick_break = 50, show_plot = TRUE, seed = 186,
  return_distr_dat = FALSE)
# e.g. # x_limits = c(250, 700),
#      # x_tick_break = 50,
```

## IS & OoS testing probability prediction accuracy: perf_eva(.,predProb.list)

probability prediction accuracy

```{r}
ProbPredAccuracy <- perf_eva(pred = predProb.list,
                             label = default.list,
                             binomial_metric = c("rmse","auc","gini"),
                             show_plot=c("roc","ks"),
                             confusion_matrix = TRUE)
```

```{r}
names(ProbPredAccuracy)
```

```{r}
ProbPredAccuracy$binomial_metric
```

```{r}
ProbPredAccuracy$confusion_matrix
```

Excursion

```{r}
perf_eva(pred = predProb.list, 
         label = default.list,
         binomial_metric = c("rmse","auc","gini"),
         show_plot= FALSE,
         confusion_matrix = FALSE)
```

## IS & OoS testing scorepoint prediction accuracy: perf_eva(.,score.list)

scorepoint prediction accuracy

```{r}
ScorePredAccuracy <- perf_eva(pred = score.list,
                             label = default.list,
                             binomial_metric = c("rmse","auc","gini"),
                             show_plot=c("roc","ks"),
                             confusion_matrix = TRUE)
```

```{r}
names(ScorePredAccuracy)
```

```{r}
ScorePredAccuracy$binomial_metric
```

```{r}
ScorePredAccuracy$confusion_matrix
```

\newpage

# Appendix

## Appendix: Essay style with formulas in LaTeX language

**Group project assignment**: Write a scholarly essay with full
sentences, correct citations and LaTeX formulas.

**Example essay style**: From a statistical perspective the transition
from the $MPS$ to the VaR framework is related to switching the
perspective from considering moments (parameters) of random variables,
i.e. $\mu$ and $\sigma$, to considering the quantiles and corresponding
probabilities of these variables. Specifically, the VaR measure
specifies the risk of a random variable ($\tilde{P}$) via the threshold
quantile ($VaR$) that is exceeded into the negative direction (i.e.
$P \leq VaR$) with the loss probability ($\alpha$) or respectively, is
exceeded into the positive direction (i.e. $P > VaR$) with the
complementary probability, i.e. the confidence level ($1-\alpha$).

## Appendix: Generating tables, figures, cross references and citations

```{r testFigure, fig.cap="Amount vs. Duration", fig.width=4, fig.height=3, fig.align='center'}
data.df[1:100,2:3] %>% plot()
```

#Figure \@ref(fig:testFigure) is a sample figure where the credit.amount
#is scatter plotted against the duration.in.month.

Formulas without numbering \begin{align*}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align*}

Formulas with numbering (and labeling which is needed for referencing)
\begin{align} \label{eq:VaR}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align}

Formula \@ref(eq:VaR) is a sample formula defining the Value at Risk.

Always cite original literature to avoid plagiarism: e.g.
\cite{SchwaigerIUF} or \citep{SchwaigerIUF}. Don't forget to cite page
numbers as well for literal citations, e.g. \cite[p. 100]{SchwaigerIUF}.

\newpage
